---
title:        "Paper 1: Instrument development"
author:       "Niklas Karlsen, OsloMet, Norway, e-mail: niklaska@oslomet.no"
date:         "2024-10-08"
bibliography: "../data/meta_data/references.bib"
csl:          "../scripts/university-of-gothenburg-apa-7th-edition-swedish-legislations.csl" # (source: https://bookdown.org/yihui/rmarkdown-cookbook/bibliography.html)
output: 
  html_document:
    toc: true
    toc_depth: 4
    toc_float: true
    number_sections: true
    
knit: (function(inputFile, encoding) { rmarkdown::render(inputFile, encoding = encoding, output_file = file.path(dirname(inputFile), '../output/Paper1.html')) })
---

# Introduction

The main purpose of this Rmd-script is to reproduce the results in paper 1. The whole process from reducing the raw data to doing the EFA and CFA is discussed. The document is inspired by the idea of a computational essay [@odden_using_2023].

This document is large. This is mainly due to the analyses done in step 5: item reduction. Especially the scatterplots to check for linearity, checks for autocorrelation and multicollinearity, and checks of the normal distribution of residuals, produce a lot of output. They are provided here for transparency. Using the menu on the left, you can skip this part if you like. 

The development of a measure usually follows three stages [@boateng_best_2018]:

1. Item development
2. Scale development
3. Scale evaluation

The first stage is described in @karlsen_assessing_2024. This computational essay describes the analysis behind the results in this paper, and emphasises the quantitative analyses done in the final two stages. This concerns item reduction and extraction of factors (EFA) in stage two, and test of dimensionality (CFA) and tests of reliability and validity in stage three. 

## Imports

These are the libraries we use for special functions.

```{r}
# libraries

### read files
library(readODS)                            # read .ods-files. Kilde: https://github.com/ropensci/readODS/
library(readxl)                             # read .xlsx-files. Kilde: https://readxl.tidyverse.org/

### presentation
library(pander)                             # print pretty tables
library(dplyr)                              # rename columns
library(tidyr)                              # pivot_longer
library(ggplot2)

### screening
library("corrplot")                         # multicollinearity
library(car)                                # vif       
library("olsrr")                            # vif
library(mice)                               # for missing 

### EFA
library(psych)                              # for skew and EFA

### CFA
library(lavaan)
library(semTools)                           # to find omega

```

# Stage 2 

From stage 2 Boateng et al. (2013) describes three steps which will be discussed here: step 4 (gather data), step 5 (item reduction) and step 6 (factor extraction, i.e., EFA). 

## Step 4: Data gathering / cleaning

This step concerns data gathering for the EFA and the CFA using separate samples for each analysis. The recommended sample size is 10 respondents per item and/or 200-300 observations in total (Boateng et al., 2018). The sample size is discussed more closely in @karlsen_assessing_2024. 

Data were collected for two pilots (EFA) and the CFA. The three datasets were reduced individually. 

The data for the first pilot were collected using two almost identical questionnaires. Four of the items were however slightly modified to use the term "coding" instead of "programming". "Programming" is used in the final published version, so this is what we choose also when reducing the dataset. 

All respondents were explicitly asked to agree to their answers being treated in the research. All answered "yes", except a few that answered blank, perhaps forgetting to check the answer, as it was not obligatory. We interpreted blank answers as "yes", although strictly speaking this should probably be interpreted as "no" to be on the safe side. For the respondents who answered blank to the qeustion to agree to participate in the research, all the questions in the questionnaire were filled in. The data contained no identifying information, and thus were anonymous. 

The raw data were downloaded from nettskjema.no in the xlsx-format and transformed to the Rdata-format in Rstudio. To get enough respondents for the EFA, the pilot data were combined with data from the learning sequence we developed (codename: emne2). 

* pilot: data-257861-2022-06-15-1302-utf.xlsx
* emne2: predata-257846-2022-03-28-1311-utf.ods
* trelis-q pilot: Datafil-utproving-trelis-q-institute4-institute1-aug2022.xlsx
* TRELIS-Q (dec-feb): data-trelis-q-tatt-ut-2feb2023.xlsx

Pilot is the pilot for the questionnaire published in @karlsen_assessing_2024. trelis-q pilot is the pilot for the TRELIS-Q questionnaire. The pilot was incorporated into TRELIS-Q. TRELIS-Q is a larger questionnaire containing questions related to the three working packages of TRELIS related to inquiry, programming and professional development. My work is related to the programming work package (WP5). The emne2-dataset was saved in the ods-format as the participant names were anonymized. 

The aim is to create the following dataset:

* data_TPACK_pilot, which uses TK, CK, etc for column names
* data_TRELIS_Q

data_TPACK_pilot will be used for the EFA, while data_TRELIS_Q will be used for the CFA.

The dataframe is built up with the different datasets as follows:

* df[1:21,] =   emne2_pre        (n=21)  
* df[22:41,] =  emne2_post       (n=20)  
* df[42:57,] =  pilot            (n=16)  
* df[58:105,] = pilot trelis_q   (n=48)  

### Load datasets

```{r load_emne2}
folder_emne2         <- "../data/rawdata/"

pre_emne2            <- "predata-257846-2022-03-28-1311-utf.ods"
post_emne2           <- "postdata-257846-2022-05-10-1048-utf.ods"

path_pre_emne2       <- paste(folder_emne2, pre_emne2, sep='')
path_post_emne2      <- paste(folder_emne2, post_emne2, sep='')
 
data_pre_emne2       <- read_ods(path_pre_emne2)
data_post_emne2      <- read_ods(path_post_emne2)
```

```{r load_pilot}
folder_pilot          <- "../data/rawdata/"
pilot                 <- "data-257861-2022-06-15-1302-utf.xlsx"

path_pilot            <- paste(folder_pilot, pilot, sep='')

data_pilot            <- read_excel(path_pilot)
```

```{r load_pilot_trelis_q}
folder_pilot_trelis_q <- "../data/rawdata/"
pilot_trelis_q        <- "Datafil-utproving-trelis-q-institute4-institute1-aug2022.xlsx"

path_pilot_trelis_q   <- paste(folder_pilot_trelis_q, pilot_trelis_q, sep='')

data_pilot_trelis_q   <- read_excel(path_pilot_trelis_q, sheet=1)
```

```{r load_TRELIS_Q}
folder_trelis_q       <- "../data/rawdata/"
trelis_q              <- "data-trelis-q-tatt-ut-2feb2023.xlsx"

path_trelis_q         <- paste(folder_trelis_q, trelis_q, sep='')

data_trelis_q         <- read_excel(path_trelis_q) #, sheet=2)
```

### Data cleaning

Clean the data in the different datasets, so they can be more easily compared. Differences in question formulations between datasets are adjusted so they are the same. Missing information (covariates) is added when it is known. 

#### emne2

```{r emne2_cleaning}
# keep some columns
data_pre_emne2  <- rename(data_pre_emne2,
                             "Subject" =
                             "Hva heter du (fornavn og etternavn)?")
data_post_emne2 <- rename(data_post_emne2,
                             "Subject" =
                             "Hva heter du (fornavn og etternavn)?")
data_pre_emne2  <- rename(data_pre_emne2,
                             "Trinn" =
                             "Går du på grunnskolelærerutdanning for trinn 1-7 eller trinn 5-10?")
data_post_emne2 <- rename(data_post_emne2,
                             "Trinn" =
                             "Går du på grunnskolelærerutdanning for trinn 1-7 eller trinn 5-10?")
data_pre_emne2  <- rename(data_pre_emne2,
                             "andre_fag" =
                             "Hva er ditt andre fag?")
data_post_emne2 <- rename(data_post_emne2,
                             "andre_fag" =
                             "Hva er ditt andre fag?")

# add institute and semester
data_pre_emne2$studiested      <- rep("Institute4",21)
data_pre_emne2$semester        <- rep(8,21)      # 8. semester (syklus 2, vår)
data_post_emne2$studiested     <- rep("Institute4",20)
data_post_emne2$semester       <- rep(8,20)      # 8. semester

# add full_treatment; 1 = deltok på all undervisning; 0 = ikke all undv.
data_pre_emne2$full_treatment  <- rep(1,21)
data_post_emne2$full_treatment <- rep(1,20)
# de som ikke deltok på alt
data_pre_emne2[data_pre_emne2$Subject   == "Ester","full_treatment"]  <- 0
data_pre_emne2[data_pre_emne2$Subject   == "Harald","full_treatment"] <- 0
data_pre_emne2[data_pre_emne2$Subject   == "Olava","full_treatment"]  <- 0
data_post_emne2[data_post_emne2$Subject == "Ester","full_treatment"]  <- 0
data_post_emne2[data_post_emne2$Subject == "Harald","full_treatment"] <- 0
data_post_emne2[data_post_emne2$Subject == "Olava","full_treatment"]  <- 0

# add gender; 0 = male; 1 = female
data_pre_emne2$Gender <- c("Kvinne", "Mann", "Kvinne", "Mann", "Kvinne", "Mann", "Kvinne", "Kvinne", "Kvinne", "Kvinne", "Mann", "Mann", "Kvinne", "Mann", "Kvinne", "Mann", "Kvinne", "Kvinne", "Kvinne", "Kvinne", "Kvinne")
data_post_emne2$Gender <-c("Kvinne", "Mann", "Mann", "Mann", "Mann", "Kvinne", "Kvinne", "Kvinne", "Kvinne", "Kvinne", "Kvinne", "Kvinne", "Kvinne", "Mann", "Kvinne", "Kvinne", "Mann", "Mann", "Kvinne", "Kvinne")

# bytt rekkefølgen på noen items
data_pre_emne2  <- data_pre_emne2  %>% relocate(Trinn,      .before = "Subject")
data_post_emne2 <- data_post_emne2 %>% relocate(Trinn,      .before = "Subject")
data_pre_emne2  <- data_pre_emne2  %>% relocate(Gender,     .after  = "Subject")
data_post_emne2 <- data_post_emne2 %>% relocate(Gender,     .after  = "Subject")
data_pre_emne2  <- data_pre_emne2  %>% relocate(andre_fag,  .before = "Subject")
data_post_emne2 <- data_post_emne2 %>% relocate(andre_fag,  .before = "Subject")
data_pre_emne2  <- data_pre_emne2  %>% relocate(studiested, .before = "Trinn")
data_post_emne2 <- data_post_emne2 %>% relocate(studiested, .before = "Trinn")
data_pre_emne2  <- data_pre_emne2  %>% relocate(semester,   .before = "Trinn")
data_post_emne2 <- data_post_emne2 %>% relocate(semester,   .before = "Trinn")
data_pre_emne2  <- data_pre_emne2  %>% relocate(full_treatment, .before = "studiested")
data_post_emne2 <- data_post_emne2 %>% relocate(full_treatment, .before = "studiested")
```

Rename items

Rename items using the term "coding" so they use "programming" instead to enable merging of the datasets.

Item labels are given based on the intended TPACK-structure when the items were created. 

```{r emne2_rename_items}
ren_emne2 <- function (data_name) {rename(data_name, 
"TK2"    = "Jeg kan lage algoritmer ved hjelp av penn og papir og flytdiagrammer.",
"TK1"    = "Jeg kan bruke algoritmisk tenkning når jeg lager dataprogrammer.",
"TK3"    = "Jeg kan lage dataprogrammer ved å bruke blokk-basert programmering.",        
"TK4"    = "Jeg kan lage dataprogrammer ved å bruke tekst-basert programmering.",             
"TK5"    = "Jeg kan bruke variabler, når jeg programmerer.",
"TK6"    = "Jeg kan bruke vilkår (if, else), når jeg programmerer.",
"TK7"    = "Jeg kan bruke løkker (for, while), når jeg programmerer.",
"TK8"    = "Jeg kan lage dataprogrammer som inneholder funksjoner.",
"CK1"    = "Jeg forstår det faglige innholdet i naturfag som handler om energi og materie.",                                        
"CK2"    = "Jeg forstår det faglige innholdet i naturfag som handler om jorda og livet på jorda.",                                         
"CK3"    = "Jeg forstår det faglige innholdet i naturfag som handler om kropp og helse.",                                          
"CK4"    = "Jeg forstår hvordan ulike typer teknologi virker, f.eks. hvordan teknologi kan styres med dataprogrammer.",
"CK5"    = "Jeg har kunnskaper om naturvitenskapelige tenkemåter og praksiser.",                                        
"TCK1"   = "Jeg kan programmere enkle teknologiske systemer som består av deler som virker sammen.",                                             
"TCK2"   = "Jeg kan bruke programmering og modellering til å utvikle roboter og mikrokontrollere, f.eks. micro:bit.",
"TCK3"   = "Jeg kan utvikle modeller av naturfaglige fenomener ved hjelp av programmering og simulering.",                                              
"TCK4"   = "Jeg kan bruke programmering til å utforske naturfaglige fenomener.",                                                   
"PK1"    = "Jeg vet hvordan jeg kan støtte elevenes refleksjon over egen læring.",                                              
"PK2"    = "Jeg vet hvordan jeg kan støtte elevene emosjonelt i deres læring, f.eks deres utholdenhet, evne til å arbeide målrettet og takle motgang.",
"PK3"    = "Jeg vet hvordan jeg kan støtte elevene sosialt i deres læring, f.eks deres nysgjerrighet, kreativitet og samarbeidsevne.",
"PK4"    = "Jeg vet hvordan jeg kan tilpasse utfordringer til elevene.",
"PK5"    = "Jeg vet hvordan jeg kan tilpasse tilbakemeldinger til elevene.",
"PK6"    = "Jeg vet hvordan jeg kan bruke vurdering til å støtte elevene underveis i deres læring.",
"TPK1"   = "Jeg vet hvordan jeg kan bruke programmering på måter som beriker undervisningen.",                                          
"TPK2"   = "Jeg vet hvordan jeg kan bruke programmering på måter som støtter elevenes læring.",                                           
"TPK3"   = "Jeg vet hvordan jeg kan tilpasse bruken av programmering til forskjellige undervisningsaktiviteter.",
"TPK4"   = "Jeg tenker kritisk gjennom hvordan jeg kan bruke programmering i undervisningen.",                                          
"TPK5"   = "Jeg vet hvordan jeg kan legge frem programmering på måter som motiverer for læring.",                                             
"PCK1"   = "Jeg vet hvordan jeg kan legge til rette for elevenes begrepslæring i naturfag.",                                        
"PCK2"   = "Jeg vet hvordan jeg kan støtte elevene i å utvikle kritisk tenkning og argumentasjonsferdigheter i naturfag.",
"PCK3"   = "Jeg vet hvordan jeg kan bruke utforskende arbeidsmåter i naturfag.",                                                           
"PCK4"   = "Jeg vet hvordan jeg kan bruke modellering som arbeidsmåte i naturfag.",                                    
"PCK5"   = "Jeg vet hvordan jeg kan vurdere elevenes måloppnåelse på ulike måter i naturfag.",                                            
"TPACK1" = "Jeg vet hvordan jeg kan motivere elevene for å bruke programmering til å lære naturfag, for eksempel ved å knytte det til elevenes interesser eller vise hvilken relevans det kan ha.",
"TPACK2" = "Jeg vet hvordan jeg kan bruke programmering i naturfag på måter som hjelper elevene å forstå mer av naturfaglige begreper og fenomener.",
"TPACK3" = "Jeg vet hvordan jeg kan bruke programmering i naturfag på måter som hjelper elevene å forstå mer av naturvitenskapelige tenkemåter og praksiser.",                                     
"TPACK4" = "Jeg vet hvordan jeg kan bruke programmering i naturfag til å lære elever hvordan teknologiske systemer fungerer.",
"TPACK5" = "Jeg føler meg klar til å bruke programmering til å undervise naturfag",
"XK1"    = "Jeg vet hvilke utfordringer elevene kan ha når de programmerer.",
"XK2"    = "Jeg har kunnskap om elevers ulike forutsetninger for å ta i bruk programmering.",                                          
"XK3"    = "Jeg vet hvordan programmering i naturfag bidrar til skolens samfunnsoppdrag.",                                               
"XK4"    = "Jeg kan være en ressursperson for kollegaene i skolen når det gjelder programmering i naturfag",
"XK5"    = "Jeg vet hvordan bruk av programmeringsteknologi kan påvirke læringssituasjonen i klasserommet.",
"XK6"    = "Jeg vet hvordan jeg kan lede en undervisningssituasjon i et klasserom der programmering er en sentral aktivitet.")
 }

data_pre_emne2_ren  <- ren_emne2(data_pre_emne2)
data_post_emne2_ren <- ren_emne2(data_post_emne2)

# select relevant questions
data_pre_emne2_ren  <- data_pre_emne2_ren [,5:55]
data_post_emne2_ren <- data_post_emne2_ren[,5:55]
```

The order of the two first questions was switched between emne2 and the pilot. Correct this. Use pilot as correct order.

* TK1: Jeg kan bruke algoritmisk tenkning når jeg lager kode.
* TK2: Jeg kan lage algoritmer ved hjelp av penn og papir og flytdiagrammer.

```{r emne2_switch_order_TK1_2}
data_pre_emne2_ren  <- data_pre_emne2_ren  %>% relocate(TK2, .after = TK1)
data_post_emne2_ren <- data_post_emne2_ren %>% relocate(TK2, .after = TK1)
```

#### pilot

One of the respondents chose the wrong institution, because their institution lacked amongst the options available. This was later corrected in the online questionnaire. I therefore have to change the affiliation of row 12 (NR 21571938) from Institute4 to Institute5, and "hovedfag" from Matematikk to Naturfag. (This is documented through the e-post correspondence with Institute5, which show the time this happened.)

```{r pilot_correct_entry}
# row number was found with View(data_pilot)
# column number was found with colnames(data_pilot)
data_pilot[12,50] <- "Institute5"
data_pilot[12,52] <- "Naturfag"
data_pilot[2,50]  <- "Institute7"  # Institute8 deltok ikke
```

```{r pilot_cleaning}
# rename columns
data_pilot <- rename(data_pilot,
                             "andre_fag" = 
                             "Hva er ditt hovedfag?")

data_pilot <- rename(data_pilot,
                             "Trinn" =
                             "Går du på grunnskolelærerutdanning for trinn 1-7 eller trinn 5-10 eller tar du videreutdanning?")

data_pilot <- rename(data_pilot,
                             "studiested" = 
                             "Hvilket studiested studerer du ved?")

# add semester
data_pilot$semester                                        <- rep(0,16)
data_pilot$semester[data_pilot$studiested == "Institute6"] <- 6 
data_pilot$semester[data_pilot$studiested == "Institute4"] <- 4 
# Institute7/8, ?. år 2/3
data_pilot$semester[data_pilot$studiested == "Institute7"] <- 6 
data_pilot$semester[data_pilot$studiested == "Institute5"] <- 8 

# reorder some columns
data_pilot <- data_pilot %>% relocate(andre_fag,  .before = "Jeg kan bruke algoritmisk tenkning når jeg lager kode.")
data_pilot <- data_pilot %>% relocate(Trinn,      .before = "andre_fag")
data_pilot <- data_pilot %>% relocate(studiested, .before = "Trinn")
data_pilot <- data_pilot %>% relocate(semester,   .before = "Trinn")
```

```{r pilot_rename}
data_pilot_ren <- data_pilot %>% rename(
"TK1"    = "Jeg kan bruke algoritmisk tenkning når jeg lager kode.",
"TK2"    = "Jeg kan lage algoritmer ved hjelp av penn og papir og flytdiagrammer.",
"TK3"    = "Jeg kan lage kode ved å bruke blokk-basert programmering.",        
"TK4"    = "Jeg kan lage kode ved å bruke tekst-basert programmering.",             
"TK5"    = "Jeg kan bruke variabler, når jeg programmerer.",
"TK6"    = "Jeg kan bruke vilkår (if, else), når jeg programmerer.",
"TK7"    = "Jeg kan bruke løkker (for, while), når jeg programmerer.",
"TK8"    = "Jeg kan lage dataprogrammer som inneholder funksjoner. (Funksjoner kan for eksempel være noen linjer med kode som har blitt skilt ut som en egen algoritme som kan gjenbrukes, f.eks. fordi den beskriver noe som gjøres ofte i programmet.)",
"CK1"    = "Jeg forstår det faglige innholdet i naturfag som handler om energi og materie.",                                        
"CK2"    = "Jeg forstår det faglige innholdet i naturfag som handler om jorda og livet på jorda.",                                         
"CK3"    = "Jeg forstår det faglige innholdet i naturfag som handler om kropp og helse.",                                          
"CK4"    = "Jeg forstår hvordan ulike typer teknologi virker, f.eks. hvordan teknologi kan styres med dataprogrammer.",
"CK5"    = "Jeg har kunnskaper om naturvitenskapelige tenkemåter og praksiser.",                                        
"TCK1"   = "Jeg kan programmere enkle teknologiske systemer som består av deler som virker sammen.",                                             
"TCK2"   = "Jeg kan bruke programmering og modellering til å utvikle roboter og mikrokontrollere, f.eks. micro:bit.",
"TCK3"   = "Jeg kan utvikle modeller av naturfaglige fenomener ved hjelp av programmering og simulering.",                                              
"TCK4"   = "Jeg kan bruke programmering til å utforske naturfaglige fenomener.",                                                   
"PK1"    = "Jeg vet hvordan jeg kan støtte elevenes refleksjon over egen læring.",                                              
"PK2"    = "Jeg vet hvordan jeg kan støtte elevene emosjonelt i deres læring, f.eks deres utholdenhet, evne til å arbeide målrettet og takle motgang.",
"PK3"    = "Jeg vet hvordan jeg kan støtte elevene sosialt i deres læring, f.eks deres nysgjerrighet, kreativitet og samarbeidsevne.",
"PK4"    = "Jeg vet hvordan jeg kan tilpasse utfordringer til elevene.",
"PK5"    = "Jeg vet hvordan jeg kan tilpasse tilbakemeldinger til elevene.",
"PK6"    = "Jeg vet hvordan jeg kan bruke vurdering til å støtte elevene underveis i deres læring.",
"TPK1"   = "Jeg vet hvordan jeg kan bruke programmering på måter som beriker undervisningen.",                                          
"TPK2"   = "Jeg vet hvordan jeg kan bruke programmering på måter som støtter elevenes læring.",                                           
"TPK3"   = "Jeg vet hvordan jeg kan tilpasse bruken av programmering til forskjellige undervisningsaktiviteter.",
"TPK4"   = "Jeg tenker kritisk gjennom hvordan jeg kan bruke programmering i undervisningen.",                                          
"TPK5"   = "Jeg vet hvordan jeg kan legge frem programmering på måter som motiverer for læring.",                                             
"PCK1"   = "Jeg vet hvordan jeg kan legge til rette for elevenes begrepslæring i naturfag.",                                        
"PCK2"   = "Jeg vet hvordan jeg kan støtte elevene i å utvikle kritisk tenkning og argumentasjonsferdigheter i naturfag.",
"PCK3"   = "Jeg vet hvordan jeg kan bruke utforskende arbeidsmåter i naturfag.",                                                           
"PCK4"   = "Jeg vet hvordan jeg kan bruke modellering som arbeidsmåte i naturfag.",                                    
"PCK5"   = "Jeg vet hvordan jeg kan vurdere elevenes måloppnåelse på ulike måter i naturfag.",                                            
"TPACK1" = "Jeg vet hvordan jeg kan motivere elevene for å bruke programmering til å lære naturfag, for eksempel ved å knytte det til elevenes interesser eller vise hvilken relevans det kan ha.",
"TPACK2" = "Jeg vet hvordan jeg kan bruke programmering i naturfag på måter som hjelper elevene å forstå mer av naturfaglige begreper og fenomener.",
"TPACK3" = "Jeg vet hvordan jeg kan bruke programmering i naturfag på måter som hjelper elevene å forstå mer av naturvitenskapelige tenkemåter og praksiser.",                                     
"TPACK4" = "Jeg vet hvordan jeg kan bruke programmering i naturfag til å lære elever hvordan teknologiske systemer fungerer.",
"TPACK5" = "Jeg føler meg klar til å bruke programmering til å undervise naturfag",
"XK1"    = "Jeg vet hvilke utfordringer elevene kan ha når de programmerer.",
"XK2"    = "Jeg har kunnskap om elevers ulike forutsetninger for å ta i bruk programmering.",                                          
"XK3"    = "Jeg vet hvordan programmering i naturfag bidrar til skolens samfunnsoppdrag.",                                               
"XK4"    = "Jeg kan være en ressursperson for kollegaene i skolen når det gjelder programmering i naturfag",
"XK5"    = "Jeg vet hvordan bruk av programmeringsteknologi kan påvirke læringssituasjonen i klasserommet.",
"XK6"    = "Jeg vet hvordan jeg kan lede en undervisningssituasjon i et klasserom der programmering er en sentral aktivitet.")

# select TPACK-questions
data_pilot_ren <- data_pilot_ren[,3:50]
```

#### pilot trelis_q

```{r pilot_trelis_q_cleaning}
# keep some columns
data_pilot_trelis_q <- rename(data_pilot_trelis_q,
                            "Trinn" =
                            "80. Går du på grunnskolelærerutdanning for trinn 1-7 eller trinn 5-10?")
data_pilot_trelis_q <- rename(data_pilot_trelis_q,
                            "Gender" =
                            "82. Er du:")
data_pilot_trelis_q <- rename(data_pilot_trelis_q,
                            "studiested" =
                            "79. Hvilket studiested studerer du ved?")
data_pilot_trelis_q <- rename(data_pilot_trelis_q,
                            "semester" =
                            "81. Hvilket studieår er du i?")

# change from year to semester
data_pilot_trelis_q$semester <- rep(7, 52)  # 4 år =  7. semester (høst)

# reorder some columns
data_pilot_trelis_q <- data_pilot_trelis_q %>% relocate(Trinn,      .before = "58. Jeg forstår sentrale begreper i naturfag innen temaer som energi og materie, jorda og livet på jorda, kropp og helse osv.")
data_pilot_trelis_q <- data_pilot_trelis_q %>% relocate(Gender,     .before = "58. Jeg forstår sentrale begreper i naturfag innen temaer som energi og materie, jorda og livet på jorda, kropp og helse osv.")
data_pilot_trelis_q <- data_pilot_trelis_q %>% relocate(studiested, .before = "Trinn")
data_pilot_trelis_q <- data_pilot_trelis_q %>% relocate(semester,   .before = "Trinn")

# remove blank rows / NA
# kilde: https://www.statology.org/remove-rows-in-r/
# select columns:
# kilde: https://www.statology.org/r-select-columns-by-index/
data_pilot_trelis_q        <- data_pilot_trelis_q[ , 54:77]

# select rows: (fjern to nederste rader med gjennomsnitt og standardavvik)
data_pilot_trelis_q        <- data_pilot_trelis_q[ 1:48, ]

# convert into numbers
data_pilot_trelis_q[,5:24] <- apply(data_pilot_trelis_q[,5:24], 2,
                    function(x) as.numeric(as.character(x)))
```

```{r pilot_trelis_q_rename}
# rename columns: (bruker dplyr)
# kilde: https://www.sharpsightlabs.com/blog/rename-columns-in-r/
data_pilot_trelis_q_ren <- rename(data_pilot_trelis_q, 
'CK123'  = '58. Jeg forstår sentrale begreper i naturfag innen temaer som energi og materie, jorda og livet på jorda, kropp og helse osv.',
"CK6"    = "59. Jeg har kunnskap om hvordan forskere i naturvitenskap arbeider",
'CK5'    = '60. Jeg har kunnskaper om naturvitenskapelige tenkemåter og praksiser',
'PCK1'   = '61. Jeg vet hvordan jeg kan legge til rette for elevenes begrepslæring i naturfag',
'PCK2'   = '62. Jeg vet hvordan jeg kan støtte elevene i å utvikle kritisk tenkning og argumentasjonsferdigheter i naturfag',
'TK2'    = '63. Jeg kan lage algoritmer ved hjelp av penn og papir og flytdiagrammer.',
'TK5'    = '64. Jeg kan bruke variabler når jeg programmerer',
'TK6'    = '65. Jeg kan bruke vilkår (if, else), når jeg programmerer',
'TCK3'   = '66. Jeg kan utvikle modeller av naturfaglige fenomener ved hjelp av programmering og simulering',
'TCK4'   = '67. Jeg kan bruke programmering til å utforske naturfaglige fenomener',
'PK3'    = '68. Jeg kan støtte elevene sosialt i deres læring, f.eks deres nysgjerrighet, kreativitet og samarbeidsevne',
'PK1'    = '69. Jeg vet hvordan jeg kan støtte elevenes refleksjon over egen læring',
'PK6'    = '70. Jeg kan bruke vurdering underveis til å støtte elevene i deres læring',
'PCK4'   = '71. Jeg vet hvordan jeg kan bruke modellering som arbeidsmåte i naturfag',
'TPK1'   = '72. Jeg vet hvordan jeg kan bruke programmering på måter som beriker undervisningen',
'TPK4'   = '73. Jeg tenker kritisk gjennom hvordan jeg kan bruke programmering i undervisningen',
'TPK5'   = '74. Jeg vet hvordan jeg kan legge frem programmering på måter som motiverer for læring',
'TPACK2' = '75. Jeg vet hvordan jeg kan bruke programmering i naturfag på måter som hjelper elevene å forstå mer av naturfaglige begreper og fenomener',
'TPACK3' = '76. Jeg vet hvordan jeg kan bruke programmering i naturfag på måter som hjelper elevene å forstå naturvitenskapelige praksiser og tenkemåter.',
'TPACK4' = '77. Jeg vet hvordan jeg kan bruke programmering i naturfag til å lære elever hvordan teknologiske systemer fungerer.')
```

#### TRELIS-Q

```{r TRELIS_Q_cleaning}
# only keep relevant columns
data_trelis_q <- data_trelis_q[,51:74]

# keep some columns; covariates
# 68. Hvilket studiested studerer du ved?	
# 69. Går du på grunnskolelærerutdanning for trinn 1-7 eller trinn 5-10?	
# 70. Hvilket studieår er du i?	
# 71. Er du:

# likert-spørsmål: 48 - 66
# åpne spørsmål: 67

data_trelis_q <- rename(data_trelis_q,
                            "Trinn" =
                            "69. Går du på grunnskolelærerutdanning for trinn 1-7 eller trinn 5-10?")
data_trelis_q <- rename(data_trelis_q,
                            "Gender" =
                            "71. Er du:")
data_trelis_q <- rename(data_trelis_q,
                            "studiested" =
                            "68. Hvilket studiested studerer du ved?")
data_trelis_q <- rename(data_trelis_q,
                            "semester" =
                            "70. Hvilket studieår er du i?")

# change from year to semester
#data_trelis_q$semester <- rep(7, 52)  # 4 år =  7. semester (høst)

# reorder some columns
data_trelis_q <- data_trelis_q %>% relocate(Trinn, .before = "48. Jeg forstår sentrale begreper i naturfag innen temaer som energi og materie, jorda og livet på jorda, kropp og helse osv.")
data_trelis_q <- data_trelis_q %>% relocate(Gender, .before = "48. Jeg forstår sentrale begreper i naturfag innen temaer som energi og materie, jorda og livet på jorda, kropp og helse osv.")
data_trelis_q <- data_trelis_q %>% relocate(studiested, .before = "Trinn")
data_trelis_q <- data_trelis_q %>% relocate(semester, .before = "Trinn")
```

```{r TRELIS_Q_rename}
# rename columns: (bruker dplyr)
# kilde: https://www.sharpsightlabs.com/blog/rename-columns-in-r/
data_trelis_q_ren <- rename(data_trelis_q, 
'CK123'  = '48. Jeg forstår sentrale begreper i naturfag innen temaer som energi og materie, jorda og livet på jorda, kropp og helse osv.',
'CK4'    = "49. Jeg forstår teknologiske prinsipper og virkemåter som er relevante for naturfag",
'CK5'    = '50. Jeg har kunnskaper om naturvitenskapelige praksiser og tenkemåter',
'PCK2'   = '51. Jeg vet hvordan jeg kan støtte elevene i å utvikle kritisk tenkning og argumentasjonsferdigheter i naturfag',
'TK2'    = "52. Jeg kan sette opp en algoritme når jeg programmerer",
'TK1a'   = "53. Jeg kan bryte ned problemer i mindre deler når jeg programmerer",
'TK34'   = "54. Jeg har kunnskap om forskjellige programmeringsverktøy, f.eks. Makecode for Micro:bit, Scratch, Python, og lignende",
'TK1b'   = "55. Jeg kan oppdage og rette feil, når jeg programmerer.",
'TK567'  = "56. Jeg vet hvordan jeg lager kode som inneholder f.eks. variabler, vilkår (if, else) og løkker (for, while)",
'TCK1'   = "58. Jeg kan bruke programmering til å utvikle enkle teknologiske systemer som består av deler som virker sammen.",
'TCK4'   = '57. Jeg kan bruke programmering til å utforske naturfaglige fenomener',
'PK3'    = '59. Jeg kan støtte elevene sosialt i deres læring i ulike fag, f.eks deres nysgjerrighet, kreativitet og samarbeidsevne',
'PK1'    = '60. Jeg vet hvordan jeg kan støtte elevenes refleksjon over egen læring i ulike fag',
'PCK5'   = '61. Jeg vet hvordan jeg kan vurdere elevenes læring på ulike måter i naturfag',
'TPK1'   = '62. Jeg vet hvordan jeg kan bruke programmering på måter som beriker undervisningen i ulike fag',
'TPK4'   = '63. Jeg tenker kritisk igjennom hvordan jeg kan bruke programmering i undervisningen i ulike fag',
'TPACK2' = '64. Jeg vet hvordan jeg kan bruke programmering i naturfag på måter som hjelper elevene å forstå naturfaglige begreper og fenomener',
'TPACK3' = '65. Jeg vet hvordan jeg kan bruke programmering i naturfag på måter som hjelper elevene å forstå naturvitenskapelige praksiser og tenkemåter.',
'TPACK4' = '66. Jeg vet hvordan jeg kan bruke programmering i naturfag på måter som hjelper elevene å forstå hvordan teknologiske systemer fungerer')

# change "5 Stemmer veldig godt" and "1 Stemmer veldig dårlig" to 5 and 1
data_trelis_q_ren[data_trelis_q_ren == '1 Stemmer veldig dårlig'] <- "1"
data_trelis_q_ren[data_trelis_q_ren == '5 Stemmer veldig godt']   <- "5"

# hva gjør jeg med 9 / VET IKKE? Gjør om til NA?
# gjør om til NA; OBS! Ikke gjør disse NA om til tall med impute, pmm senere
data_trelis_q_ren[data_trelis_q_ren == 'VET IKKE'] <- NA # "9"

# convert into numbers
data_trelis_q_ren[,5:23] <-  apply(data_trelis_q_ren[,5:23], 2,
                    function(x) as.numeric(as.character(x)))
```

```{r rename_dataset}
data_TRELIS_Q <- data_trelis_q_ren
```

```{r}
# remove open-ended question column
data_TRELIS_Q <- data_TRELIS_Q[,1:23]
```

```{r}
# readxls stores as tibble, not dataframe?
# make tibble into data.frame to treat it similarly to pilot dataset
data_TRELIS_Q <- data.frame(data_TRELIS_Q)
```

#### merge dataframes

```{r merge_data_pilot_emne2}
# begin with text-values: spring 2022
data_prepost_emne2_ren <- bind_rows(data_pre_emne2_ren,
                                    data_post_emne2_ren)
data_pilot_emne2_ren   <- bind_rows(data_prepost_emne2_ren, data_pilot_ren)
```

```{r convert_likert_to_numbers}
# data_list (enig = 1, osv.)
data_pilot_emne2_ren[data_pilot_emne2_ren == 'Veldig uenig'] <- "1"
data_pilot_emne2_ren[data_pilot_emne2_ren == 'Veldig enig']  <- "5"
data_pilot_emne2_ren[data_pilot_emne2_ren == 'Hverken enig eller uenig'] <- "3"
data_pilot_emne2_ren[data_pilot_emne2_ren == 'Uenig']        <- "2"
data_pilot_emne2_ren[data_pilot_emne2_ren == 'Enig']         <- "4"

# turn values into integer from character
# kilde: https://statisticsglobe.com/convert-data-frame-column-to-numeric-in-r
data_pilot_emne2_ren[,8:ncol(data_pilot_emne2_ren)] <- apply(
                    data_pilot_emne2_ren[,8:ncol(data_pilot_emne2_ren)], 2,
                    function(x) as.numeric(as.character(x)))
```

```{r merge_all_datasets}
data_TPACK_pilot <- bind_rows(data_pilot_emne2_ren, data_pilot_trelis_q_ren)
```

```{r identify_dataset_in_dataframe}
datasett <- c(replicate(21, "pre"),
              replicate(20, "post"),
              replicate(16, "pilot"),
              replicate(48, "pilot_trelis_q"))

data_TPACK_pilot <- cbind(datasett, data_TPACK_pilot)
```

### Save reduced datasets

Size of dataset

* emne2_pre:         `r dim(data_pre_emne2_ren)`
* emne2_post:        `r dim(data_post_emne2_ren)`
* pilot:             `r dim(data_pilot_ren)`
* pilot_trelis_q:    `r dim(data_pilot_trelis_q_ren)`

```{r save_data_pilot}
save(data_TPACK_pilot, file="../data/processed_data/tpack-pilot_2022.Rdata")
```

```{r save_data_TRELIS_Q}
save(data_TRELIS_Q, 
     file="../data/processed_data/tpack-trelis_q_partial_2023-paper1.Rdata")
```

```{r}
# clean environment to reset for next step
# ref.: https://www.statology.org/clear-environment-in-r/ [3.9.2024]
rm(list=ls())
```

## Step 5: Item reduction (assumption checking)

```{r}
do_screening <- 1 # skip screening during testing
```

```{r}
# load data
load(file="../data/processed_data/tpack-pilot_2022.Rdata") 
load(file="../data/processed_data/tpack-trelis_q_partial_2023-paper1.Rdata")
```

In the following we will only consider the pre-datasett, so we remove the post-data, which is to dissimilar from the pilot data.

```{r select_data_for_screening_step5}
# emne2_anon (uten post)
data_TPACK_pilot_minuspost <- data_TPACK_pilot[
  data_TPACK_pilot$datasett != "post" & 
  data_TPACK_pilot$datasett != "pilot_trelis_q",]

```

The aim of this step is to reduce the number of items to the lowest necessary by removing redundant items based on individual item tests. Since our measure is a self-report questionnaire, the relevant tests are based on correlations (step 5.2 and 5.3) and consideration of missing data (step 5.5). Step 5.1 and 5.4 are omitted as they concern correct or wrong answers to tests, which is not relevant for a self-report measure. 

Factor analysis is based on linear analysis (i.e., ordinary least squares-regression (OLS)). Assumptions underlying OLS-regression are according to @christophersen_introduksjon_2018 that the residuals have both an average value of 0, constant variance (i.e,. homoscedasticity) and independence (i.e., no autocorrelation). In addition, the dependent variables should depend linearly on the independent variables, which should have no multicollinearity (i.e., meaning that no residuals(?) of independent variable should be strongly correlated with any of the other independent variables). 
In addition, according to @flora_old_2012, it is important to decide whether to treat the data as ordinal or interval, as this has consequences for how the items are screened. Treating the data as interval, requires checking of missing data, linearity (checking for outliers and collinearity, i.e., very strong correlation between variables), and normally distributed residuals, checks which are not necessary to do if the data are treated as ordinal. In addition, checks suggested by @izquierdo_exploratory_2014 were done, such as check of eigenvalues, item-test correlation and reliabilities (i.e., Cronbach's alpha).

* Step 5a: check residuals (normality, autocorrelation)
* Step 5b: check correlations (collinearity, multicollinearity)
* Step 5c: check outliers (skew, kurtosis, range)
* Step 5d: check missing (pattern: MCAR, MAR, NMAR)
* Step 5e: check eigenvalues, item-test correlation, reliability [@izquierdo_exploratory_2014]

A list of items is compiled for easier retrieval.

```{r items_pilot}
TK_pos <- c("TK1","TK2","TK3","TK4","TK5","TK6","TK7","TK8")
PK_pos <- c("PK1","PK2","PK3","PK4","PK5","PK6")
CK_pos <- c("CK1","CK2","CK3","CK4","CK5")
PCK_pos <- c("PCK1","PCK2","PCK3","PCK4","PCK5")
TCK_pos <- c("TCK1","TCK2","TCK3","TCK4")
TPK_pos <- c("TPK1","TPK2","TPK3","TPK4","TPK5")
TPACK_pos <- c("TPACK1","TPACK2","TPACK3","TPACK4","TPACK5")

#TPACK_complete <- c(TK_pos, PCK_full_pos, TPACK_full_pos) # minus XK

pilot_list <- list(TK=TK_pos,
                   CK=CK_pos,
                   PK=PK_pos,
                   PCK=PCK_pos,
                   TCK=TCK_pos,
                   TPK=TPK_pos,
                   TPACK=TPACK_pos)

```

```{r items_TRELIS_Q}
TK_pos_trelisq <- c("TK1a","TK1b","TK2","TK34","TK567")
CK_pos_trelisq <- c("CK4","CK5","CK123")
PCK_pos_trelisq <- c("PCK2","PCK5")
TCK_pos_trelisq <- c("TCK1","TCK4")
PK_pos_trelisq <- c("PK1","PK3")
TPK_pos_trelisq <- c("TPK1","TPK4")
TPACK_pos_trelisq <- c("TPACK2","TPACK3","TPACK4")

TRELISQ_list <- list(TK=TK_pos_trelisq,
                     CK=CK_pos_trelisq,
                     PK=PK_pos_trelisq,
                     PCK=PCK_pos_trelisq,
                     TCK=TCK_pos_trelisq,
                     TPK=TPK_pos_trelisq,
                     TPACK=TPACK_pos_trelisq)

```

### Ordinal or interval data?

When we have as few as five answer alternatives (i.e., 5-pt Likert scale), there are weighty arguments for considering the data as ordinal [@flora_old_2012; @izquierdo_exploratory_2014; @watkins_exploratory_2018]. Five answer alternatives do however begin to imitate an interval scale (i.e., a continous scale). Thus, researchers have also argued for considering 5-pt scales as being continuous [@christophersen_introduksjon_2018; @norman_likert_2010]. In order to use the full arsenal of parametric methods (i.e., the properties of the sample are decided by a few parameters), such as maximum likelihood (ML), we decided to consider the data as interval. To support this choice further, we also presented the five alternatives as points on a linear scale in our questionnaire to help the respondents visualise the alternatives as being on an interval scale. 

### Linearity?

Here I do checks for linearity, collinearity and multicollinearity.

```{r help_functions_check_collinearity}
### HELP FUNCTIONS

lag_eigenvalue_tabell <- function(ev_list) {
  no_ev <- length(ev_list) # antall eigenvalues
  kum_var <- 0
  
  # gjør om til tabell
  ev_tabell <- data.frame(matrix(ncol=4,nrow=0))
  for (i in 1:no_ev) {
    ev <- ev_list[i]
    kum_var <- kum_var + ev/no_ev
    faktor <- cbind(as.integer(i), ev, ev/no_ev, kum_var)
    #print(faktor)
    ev_tabell <- rbind(ev_tabell, faktor)
  }
  
  colnames(ev_tabell) <- c("Factor","Eigenvalue","Variance","Cummulativ variance")
  
  pander(format(ev_tabell,digits=2))
}


collinear <- function (dataset) {
  ###
  # Christophersen (2018): r > 0.9?
  ###
  
  teller <- 0                          # hvor mange sjekker gjør jeg?
  start <- 1                           
  end <- ncol(dataset)
  i <- 1                               # start posisjon
  for (var1 in colnames(dataset[start:(end-2)])) {
    i <- i + 1                         # for å unngå duplikater
    for (var2 in colnames(dataset[i:(end-1)])) {
      # check for collinearity, r>0.90
      r <- cor.test(dataset[,var1], dataset[,var2])
      
      if (abs(r$estimate)>0.9) { 
        print("collinearity: r > 0.90")
        print(c(dataset, var1 ,var2, r$estimate))   # marker verdier større enn 0.90
       
        teller <- teller + 1   # count number of scatterplots
      }
    }
  }
  
  if (teller == 0) {
    print("Ingen kolinearitet oppdaget") # antall plot
  }

  ###
  # Flora et al. (2012): eigenvalues <= 0? condition index < 30?
  ###
  ev <- eigen(cor(dataset,use="pairwise.complete.obs"), only.values=TRUE)
  ev_table <- lag_eigenvalue_tabell (ev$values)
  #ev
  # beregn condition index: ci < 30
  ci <- ev$values[1]**2 - ev$values[length(ev$values)]**2 
  print(c("Condition index(<30?):", ci))
  return(ev_table)
}
```

#### pilot

Through an iterative process, the following items were identified as contributing to high collinearity (r>0.9). They were therefore excluded from the dataset.

```{r remove_collinear_items_pilot}
### pilot
data_TPACK_pilot_minuspost <- data_TPACK_pilot_minuspost %>%
    # 25.5.2023: velg bort items som ikke er i TRELIS-Q
    select(-c("TK8","PK2","PK4","PK5","PK6","PCK1","PCK3","PCK4","TCK2","TCK3",
              "TPK2","TPK3","TPK5","TPACK1","TPACK5",
              "TPACK3", # colinear in trelis-q; "TK1","TPK1",
              "TK6","TK4", "PK1", "PK3"))

```

##### Linearity

First I plot scatterplots for indvidiual items against their intended construct (e.g., TK2 vs all TK-items).

```{r linear_scatterplots_pilot}
# scatterplot som korrigerer for overlap mellom datapunkter
teller <- 0                          # hvor mange plot lager jeg?

end <- ncol(data_TPACK_pilot_minuspost)
i <- 1                               # start posisjon
for (var1 in names(pilot_list)) {           # TPACK-element
  items <- colnames(data_TPACK_pilot_minuspost)[colnames(data_TPACK_pilot_minuspost) %in% pilot_list[[var1]]]
  ifelse (length(items)>1, 
    y <- rowMeans(data_TPACK_pilot_minuspost[, items]), # y-variabel
    y <- data_TPACK_pilot_minuspost[, items])           # y består av bare ett item
  i <- i + 1                         # for å unngå duplikater
  for (var2 in items) {               # x-variabel
    # scatterplot
    # kilde: https://ggplot2-book.org/statistical-summaries.html
    # kilde: https://stackoverflow.com/questions/32004292/r-ggplot-for-loop-aes-variable
    x <- data_TPACK_pilot_minuspost[,var2]
    xy <- data.frame(t(rbind(y,x)))
    colnames(xy) <- c(var1,var2)
      
    # check for collinearity, r>0.90
    r <- cor.test(x,y)
      
    # 0.4 er moderat korrelasjon ift Christophersen (2018) kap. 3
    if (abs(r$estimate)>0.4) { # only plot scatterplots which may be linear?
      xy_plot <- ggplot(xy, aes_string(var2,var1)) +
        geom_point() +                            # shape=1,alpha=1/5
        geom_jitter() +
        ggtitle(paste("scatterplot", data_TPACK_pilot_minuspost$datasett))

      print(xy_plot)
    teller <- teller + 1   # count number of scatterplots
    }
  }
}
print(teller) # antall plot
```

Then I plot scatterplots for the average value of each intended construct against all the individual items for each intended construct (e.g., average of all TK-items against individual values of all TK-items).

```{r linear_scatterplots_aggregated_pilot}
teller <- 0                          # hvor mange plot lager jeg?

end <- ncol(data_TPACK_pilot_minuspost)
i <- 1                               # start posisjon
for (var1 in names(pilot_list)) {           # TPACK-element
  items <- colnames(data_TPACK_pilot_minuspost)[colnames(data_TPACK_pilot_minuspost) %in% pilot_list[[var1]]]
    
  if (length(items) == 0) {next} # skip empty items
    
  ifelse (length(items)>1,  # calculate average value
    y <- rowMeans(data_TPACK_pilot_minuspost[, items]), # y-variabel
    y <- data_TPACK_pilot_minuspost[, items])

  x <- data_TPACK_pilot_minuspost[, items]
    
  xy <- data.frame(y,x)
  colnames(xy) <- c(var1,items)
    
  # pivot_longer
  # Warning: Using an external vector in selections was deprecated in tidyselect 1.1.0.
  # ℹ Please use `all_of()` or `any_of()` instead.
  xy_long <- pivot_longer(xy,!c(all_of(var1)), names_to="items",values_to = "likert")
    
  #Warning: `aes_string()` was deprecated in ggplot2 3.0.0.
  #ℹ Please use tidy evaluation idioms with `aes()`.
  #ℹ See also `vignette("ggplot2-in-packages")` for more information.

  xy_plot <- ggplot(xy_long, aes_string("likert",var1)) +
    geom_point() +                            # shape=1,alpha=1/5
    geom_jitter() +
    ggtitle(paste("scatterplot", data_TPACK_pilot_minuspost$datasett))

  print(xy_plot)
  teller <- teller + 1   # count number of scatterplots
}

print(teller) # antall plot
```

##### Collinearity

Eigenvalues are examined for possible collinearity. Eigenvalues should be significantly above zero. 

```{r collinearity_pilot}
# eigenvalue_table
if (do_screening == 1) {

ev_table <- collinear(data_TPACK_pilot_minuspost[,colnames(data_TPACK_pilot_minuspost) %in% unlist(pilot_list)])
print(ev_table)
}
```

##### Multicollinearity

```{r check_multicollinearity_pilot}
# multiple regression model

if (do_screening == 1) {

# I browse through the items, and remove the current item by subtracting it at the end of the formula
mc <- NULL      # list of items with multicollinearity
    
# kilde: https://www.codingprof.com/3-ways-to-test-for-multicollinearity-in-r-examples/
corrplot(cor(data_TPACK_pilot_minuspost[,colnames(data_TPACK_pilot_minuspost) %in% unlist(pilot_list)]), method = "number", number.cex=0.3) # r bør være under 0.85

# forenkle navn på dataset for prettyprint
df <- data_TPACK_pilot_minuspost

#items <- unlist(pilot_list)
items <- colnames(df[,colnames(df) %in% unlist(pilot_list)]) # ikke ha med subset-kolonnen  

# lag tekststreng for uavhengige variable
#antall_tpack <- length(grep("TPACK",items))
independent_var <- paste("'] ~ df [,'", items[1], sep="") #TK1"
for (item in items[2:(length(items))]) { #-antall_tpack)]) { # omit TPACK
  independent_var <- paste(independent_var, "'] + df [,'", item, sep="")
}
independent_var <- paste(independent_var, "'] - df  [,'", sep="")
      
for (item in items) {
  model <- lm( as.formula(
  paste( "df [,'", item, independent_var, item, "']", sep="")))
     
  print("############################")
  print("######## NEW ITEM ##########")
  print("############################")
        
  print(item)

  # check for multicollinearity: R > 0.9
  print("Any R>0.9?")
  mc_items <- model$coefficients[abs(model$coefficients)>0.9]
  if (length(mc_items) > 0) {
    print(c("multicollinearitet R > 0.9?", mc_items))
    print(summary(model))
    mc <- c(mc, item)
  }

  #if (vif == 1) {
  # kilde: https://www.statology.org/multicollinearity-in-r/
  print("car::vif")
  print(vif(model))

  print("olsrr::vif")
  # kilde: https://www.codingprof.com/3-ways-to-test-for-multicollinearity-in-r-examples/
  print("tolerance bør være over 0.1 og VIF under 10.")
  print(ols_vif_tol(model)) # tolerance bør være over 0.1 og VIF under 10.
  #print("CI bør både være under 10 og varians lav")
  #print(ols_eigen_cindex(model)) # CI bør både være under 10 og varians lav
}

# print items with multicollinearity
print(mc)
}
```

No multicollinearity. The items added to mc are due to the intercepts, not the slopes. 

#### TRELIS_Q

```{r remove_collinear_items_pilot_TRELISQ}
### pilot trelis_q
data_TPACK_trelis_q <- data_TRELIS_Q %>%
    select(-c("TPACK3"))
```

##### Linearity

```{r linear_scatterplots_TRELISQ}
# scatterplot som korrigerer for overlap mellom datapunkter
teller <- 0                          # hvor mange plot lager jeg?

end <- ncol(data_TPACK_trelis_q)
i <- 1                               # start posisjon
for (var1 in names(TRELISQ_list)) {           # TPACK-element
  items <- colnames(data_TPACK_trelis_q)[colnames(data_TPACK_trelis_q) %in% TRELISQ_list[[var1]]]
  ifelse (length(items)>1, 
    y <- rowMeans(data_TPACK_trelis_q[, items]), # y-variabel
    y <- data_TPACK_trelis_q[, items])           # y består av bare ett item
  i <- i + 1                         # for å unngå duplikater
  for (var2 in items) {               # x-variabel
    # scatterplot
    # kilde: https://ggplot2-book.org/statistical-summaries.html
    # kilde: https://stackoverflow.com/questions/32004292/r-ggplot-for-loop-aes-variable
    x <- data_TPACK_trelis_q[,var2]
    xy <- data.frame(t(rbind(y,x)))
    colnames(xy) <- c(var1,var2)
      
    # check for collinearity, r>0.90
    r <- cor.test(x,y)
      
    # 0.4 er moderat korrelasjon ift Christophersen (2018) kap. 3
    if (abs(r$estimate)>0.4) { # only plot scatterplots which may be linear?
      xy_plot <- ggplot(xy, aes_string(var2,var1)) +
        geom_point() +                            # shape=1,alpha=1/5
        geom_jitter() +
        ggtitle(paste("scatterplot", data_TPACK_trelis_q$datasett))

      print(xy_plot)
    teller <- teller + 1   # count number of scatterplots
    }
  }
}
print(teller) # antall plot
```

```{r linear_scatterplots_aggregated_TRELISQ}
teller <- 0                          # hvor mange plot lager jeg?
#datasett_index <- 1
#start <- 1                          # siste item = datasett

end <- ncol(data_TPACK_trelis_q)
i <- 1                               # start posisjon
for (var1 in names(TRELISQ_list)) {           # TPACK-element
  items <- colnames(data_TPACK_trelis_q)[colnames(data_TPACK_trelis_q) %in% TRELISQ_list[[var1]]]
    
  if (length(items) == 0) {next} # skip empty items
    
  ifelse (length(items)>1,  # calculate average value
    y <- rowMeans(data_TPACK_trelis_q[, items]), # y-variabel
    y <- data_TPACK_trelis_q[, items])

  x <- data_TPACK_trelis_q[, items]
    
  xy <- data.frame(y,x)
  colnames(xy) <- c(var1,items)
    
  # pivot_longer
  # Warning: Using an external vector in selections was deprecated in tidyselect 1.1.0.
  # ℹ Please use `all_of()` or `any_of()` instead.
  xy_long <- pivot_longer(xy,!c(all_of(var1)), names_to="items",values_to = "likert")
    
  #Warning: `aes_string()` was deprecated in ggplot2 3.0.0.
  #ℹ Please use tidy evaluation idioms with `aes()`.
  #ℹ See also `vignette("ggplot2-in-packages")` for more information.

  xy_plot <- ggplot(xy_long, aes_string("likert",var1)) +
    geom_point() +                            # shape=1,alpha=1/5
    geom_jitter() +
    ggtitle(paste("scatterplot", data_TPACK_trelis_q$datasett))

  print(xy_plot)
  teller <- teller + 1   # count number of scatterplots
}

print(teller) # antall plot
```

##### Collinearity

```{r check_collinearity_TRELISQ}
# eigenvalue_table

if (do_screening == 1) {
  ev_table <- collinear(data_TPACK_trelis_q[,colnames(data_TPACK_trelis_q) %in% unlist(TRELISQ_list)])
  print(ev_table)
}
```

##### Multicollinearity

```{r check_multicollinearity_TRELISQ}
# multiple regression model

if (do_screening == 1) {

  # I browse through the items, and remove the current item by subtracting it at the end of the formula
mc <- NULL      # list of items with multicollinearity
    
# kilde: https://www.codingprof.com/3-ways-to-test-for-multicollinearity-in-r-examples/
corrplot(cor(data_TPACK_trelis_q[,colnames(data_TPACK_trelis_q) %in% unlist(TRELISQ_list)]), method = "number", number.cex=0.3) # r bør være under 0.85

# forenkle navn på dataset for prettyprint
df <- data_TPACK_trelis_q

#items <- unlist(TRELISQ_list)
items <- colnames(df[,colnames(df) %in% unlist(TRELISQ_list)]) # ikke ha med subset-kolonnen  

# lag tekststreng for uavhengige variable
#antall_tpack <- length(grep("TPACK",items))
independent_var <- paste("'] ~ df [,'", items[1], sep="") #TK1"
for (item in items[2:(length(items))]) { #-antall_tpack)]) { # omit TPACK
  independent_var <- paste(independent_var, "'] + df [,'", item, sep="")
}
independent_var <- paste(independent_var, "'] - df  [,'", sep="")
      
for (item in items) {
  model <- lm( as.formula(
  paste( "df [,'", item, independent_var, item, "']", sep="")))
     
  print("############################")
  print("######## NEW ITEM ##########")
  print("############################")
        
  print(item)

  # check for multicollinearity: R > 0.9
  print("Any R>0.9?")
  mc_items <- model$coefficients[abs(model$coefficients)>0.9]
  if (length(mc_items) > 0) {
    print(c("multicollinearitet R > 0.9?", mc_items))
    print(summary(model))
    mc <- c(mc, item)
  }

  #if (vif == 1) {
  # kilde: https://www.statology.org/multicollinearity-in-r/
  print("car::vif")
  print(vif(model))

  print("olsrr::vif")
  # kilde: https://www.codingprof.com/3-ways-to-test-for-multicollinearity-in-r-examples/
  print("tolerance bør være over 0.1 og VIF under 10.")
  print(ols_vif_tol(model)) # tolerance bør være over 0.1 og VIF under 10.
  #print("CI bør både være under 10 og varians lav")
  #print(ols_eigen_cindex(model)) # CI bør både være under 10 og varians lav
}

# print items with multicollinearity
print(mc)

}
```

### Outliers?

Outliers may impact calculations of standard deviations. Flora et al. (2012) and Watkins (2018) recommend, however, not to remove outliers. Christophersen (2018) recommends to compare calculations with and without outliers to determine their significance. 

```{r help_function_outlier}
### HELP FUNCTION
# describe data (mean, sd, skew, curtosis)
# requires library(moments) for skew
describe_data <- function (datalist, QUIET=1) {

  im  <- mean(datalist, na.rm=TRUE)
  isd <- sd(datalist, na.rm=TRUE)
  isk <- psych::skew(datalist, na.rm=TRUE) # from psych
  ik  <- kurtosi(datalist, na.rm=TRUE) # from psych
  mis <- sum(is.na(datalist)) # missing
  ir  <- max(datalist, na.rm=TRUE) - min(datalist, na.rm=TRUE) # range
  
  # source: https://stackoverflow.com/questions/34351598/getting-name-of-variable-in-r
  if (QUIET != 1) {
    print(sprintf("%7s = %s",   "Item",    deparse(substitute(datalist))))
    print(sprintf("%7s = %.3f", "mean",    im))
    print(sprintf("%7s = %.3f", "std.dev", isd))
    print(sprintf("%7s = %.3f", "skew",    isk))
    print(sprintf("%7s = %.3f", "kurt",    ik))
    print(sprintf("%7s = %.3f", "range",   ir))
    print(sprintf("%7s = %i",   "missing", mis))

    if (isk > 2 | ik > 7) {
      print("                            Skew or curtosis TOO LARGE.")
    }
  }
  
  # outliers
  lowerq = quantile(datalist, na.rm=TRUE)[2]
  upperq = quantile(datalist, na.rm=TRUE)[4]
  iqr    = upperq - lowerq               #Or use IQR(df)
  # gang med 3 for extreme outliers, 1.5 for mild outliers
  
  mild.threshold.upper = (iqr * 3) + upperq
  mild.threshold.lower = lowerq - (iqr * 3)
  
  # print outliers
  # kilde: https://stackoverflow.com/questions/19353116/how-do-i-print-values-in-a-list-that-are-greater-than-a-certain-number-along-wit#19353192
  outliers_upper <- which(datalist>mild.threshold.upper)
  outliersu      <- datalist[outliers_upper]
  outliers_lower <- which(datalist<mild.threshold.lower)
  outliersl      <- datalist[outliers_lower]
  
  if (length(outliersu) > 0 | length(outliersl) > 0) {
    print("outliers upper =")
    print(outliersu)
    print("outliers lower =")
    print(outliersl)
  }
  
  #hist (datalist, main=i) # histogram
  #readline(prompt = "Press enter")
  
  return(c("mean"=round(im,3), "skew"=round(isk,3), "kurtosi"=round(ik,3)))
           #round(outliersu,3), round(outliersl,3)))
}

```

#### pilot

```{r outlier_pilot}
# skip first column which is list of subsets
#datasett_index <- 1
#for (datasett in names(data_list)) {
  for (i in colnames(data_TPACK_pilot_minuspost)[colnames(data_TPACK_pilot_minuspost) %in% unlist(pilot_list)]) {
    print(paste(i))
    describe_data(data_TPACK_pilot_minuspost[,i])
  }
#  datasett_index <- datasett_index + 1
#}

print(describe(data_TPACK_pilot_minuspost)) # independent variables, x
#print(describe(scoreItems(pilot_list,data_TPACK_pilot_minuspost)$scores)) # dependent variables
```

#### TRELIS_Q

```{r outlier_TRELISQ}
# skip first column which is list of subsets
#datasett_index <- 1
#for (datasett in names(data_list)) {
  for (i in colnames(data_TPACK_trelis_q)[colnames(data_TPACK_trelis_q) %in% unlist(TRELISQ_list)]) {
    print(paste(i))
    describe_data(data_TPACK_trelis_q[,i])
  }
#  datasett_index <- datasett_index + 1
#}

print(describe(data_TPACK_trelis_q)) # independent variables, x
#print(describe(scoreItems(TRELISQ_list,data_TPACK_trelis_q)$scores)) # dependent variables
```

### Normal distribution of residuals?

Statistical tests assume the residuals are normally distributed. Common criteria are that skewness should be below 2 or 3, and that curtosis (i.e., flatness, which is related to outliers) should be below 7 or 10 (Flora et al., 2012; Christophersen, 2018). 

y = ax + b + restledd

Residuals should have an average value of 0, equal variance (homoscedasticity) and no interdependence (no autocorrelation) (Christophersen, 2018, kap. 7).

```{r help_function_residuals_singleitems}
### HELP FUNCTION

check_residuals <- function(dataset, items) {
  
  table_residuals <- NULL
  
  teller <- 0                          # hvor mange kombinasjoner sjekker jeg?
  i      <- 1                          # start posisjon
  end    <- length(items)
  for (var1 in items[1:(end-1)]) {
    ifelse (length(items)>1, 
      y  <- rowMeans(dataset[, items]), # y-variabel
      y  <- dataset[, items])           # y består av bare ett item
    
    i    <- i + 1                      # for å unngå duplikater
    for (var2 in items[i:end]) {       # x-variabel
      x  <- dataset[,var2]
      xy <- data.frame(t(rbind(y,x)))
      colnames(xy) <- c(var1,var2)
      
      model <- lm( as.formula(
      paste( "xy [,'", var1, "'] ~ xy [,'", var2, "']", sep="")))
 
      print(c(var1, "=", var2))
        
      # check for outliers in residuals
      descriptive <- (describe_data(model$residuals)) # returns ("mean"=im, "skew"=isk, "kurtosi"=ik, outliersu, outliersl)
        
      # grense for betydningsfulle outliers / leverage; s. 78 i Christophersen, 2018
      k          <- length(items) # antall uavhengige variables
      n          <- nrow(xy)
      ifelse (k<10, 
        leverage <- 3*(k+1)/n,
        leverage <- 2*(k+1)/n
      )
      #print(c("betydningsfull grense:", leverage))
        
      # check for normalfordeling
      # 1: normalfordeling av residuals?
      hist(residuals(model), col = "steelblue")
      # 2: jevn fordeling av varians, homoskedastisitet
      print(plot(fitted(model), residuals(model)))
      print(abline(h = 0, lty = 2))
         
      teller <- teller + 1   # count number of scatterplots

      # check for autocorrelation
      dw     <- durbinWatsonTest(model)
      print(dw)
      
      table_residuals <- rbind(table_residuals, c("var1"=var1, "var2"=var2, 
                                                  descriptive, 
                                                  "dw=2"=round(dw$dw,2)))
    } 
  }
  print(teller) # antall plot
  
  # gjør om til tabell
  table_residuals         <- data.frame(table_residuals)
  
  # gjør om til tall
  table_residuals$mean    <- as.numeric(table_residuals$mean)
  table_residuals$skew    <- as.numeric(table_residuals$skew)
  table_residuals$kurtosi <- as.numeric(table_residuals$kurtosi)
  table_residuals$'dw.2'  <- as.numeric(table_residuals$'dw.2')

  return (table_residuals)
}

```

#### pilot

```{r autocorrelation_pilot}
if (do_screening == 1) {

items     <- colnames(data_TPACK_pilot_minuspost)[colnames(data_TPACK_pilot_minuspost) %in% unlist(pilot_list)]
table_res <- check_residuals(data_TPACK_pilot_minuspost, items)

# print items with large skew og kurtosi or autocollinearity
pander(table_res[abs(table_res$skew) > 1. | abs(table_res$kurtosi) > 3. | table_res$dw.2 > 2.5 | table_res$dw.2 < 1.5,])

}
```

Are there correlating residuals?  
Autocorrelation: Durbin-Watsons d = 2*(1-r)  

If d = 2, then OK  
If d < 1 eller d > 3, then not OK

If autocorrelation, use robust SE

Autocorrelation can be due to structured data. Then use multilevel-analysis. 

```{r distribution_residuals_aggregated_pilot}
teller         <- 0                             # hvor mange plot lager jeg?
datasett_index <- 1
#for (dataset in names(data_list)) {
#  datasett <- data_list[[dataset]]      # access data object
  for (var1 in names(pilot_list)) {     # TPACK-element
    items      <- colnames(data_TPACK_pilot_minuspost)[colnames(data_TPACK_pilot_minuspost) %in% pilot_list[[var1]]]
    ifelse (length(items)>1,            # ikke gjør analyse hvis bare ett item
      y        <- rowMeans(data_TPACK_pilot_minuspost[, items]), # y-variabel
      y        <- 0)

   print(c(var1, "=", items))
   
   if (!length(y)==1) {                # sjekk for at vi har flere enn et item
    
      x        <- data_TPACK_pilot_minuspost[, items]
    
      xy       <- data.frame(y,x)
      colnames(xy) <- c(var1,items)
    
      # lag tekststreng til formel y= x1 + x2 + osv.
      independent_var <- paste("'] ~ xy [,'", items[1], sep="") #TK1"
      for (item in items[2:(length(items))]) { 
        independent_var <- paste(independent_var, "'] + xy [,'", item, sep="")
      }
      independent_var <- paste(independent_var, "'] - xy  [,'", sep="")
      
      #for (item in items) {
        model <- lm( as.formula(
        paste( "xy [,'", item, independent_var, item, "']", sep="")))

        # check for outliers in residuals
        print(describe_data(model$residuals))
        
        # grense for betydningsfulle outliers / leverage; s. 78 i Christophersen, 2018
        k <- length(items) # antall uavhengige variables
        n <- nrow(xy)
        ifelse (k<10, 
                leverage <- 3*(k+1)/n,
                leverage <- 2*(k+1)/n
                )
         print(c("betydningsfull grense:", leverage))
        
        # check for normalfordeling
        # 1: normalfordeling av residuals?
        print(hist(residuals(model), col = "steelblue"))
        # 2: jevn fordeling av varians, homoskedastisitet
        print(plot(fitted(model), residuals(model)))
        print(abline(h = 0, lty = 2))
         
        teller <- teller + 1   # count number of scatterplots
      #}
    }
  }
  #datasett_index <- datasett_index + 1
#} 
print(teller) # antall plot
    
```

#### TRELIS_Q

```{r autocorrelation_TRELISQ}
if (do_screening == 1) {

items     <- colnames(data_TPACK_trelis_q)[colnames(data_TPACK_trelis_q) %in% unlist(TRELISQ_list)]
table_res <- check_residuals(data_TPACK_trelis_q, items)

# print items with large skew og kurtosi or autocollinearity
pander(table_res[abs(table_res$skew) > 1. | abs(table_res$kurtosi) > 3. | table_res$dw.2 > 2.5 | table_res$dw.2 < 1.5,])

}
```

Are there correlating residuals?  
Autocorrelation: Durbin-Watsons d = 2*(1-r)  

If d = 2, then OK  
If d < 1 eller d > 3, then not OK

If autocorrelation, use robust SE

Autocorrelation can be due to structured data. Then use multilevel-analysis. 

```{r distribution_residuals_aggregated_TRELISQ}
teller         <- 0                             # hvor mange plot lager jeg?
datasett_index <- 1
#for (dataset in names(data_list)) {
#  datasett <- data_list[[dataset]]      # access data object
  for (var1 in names(TRELISQ_list)) {     # TPACK-element
    items      <- colnames(data_TPACK_trelis_q)[colnames(data_TPACK_trelis_q) %in% TRELISQ_list[[var1]]]
    ifelse (length(items)>1,            # ikke gjør analyse hvis bare ett item
      y        <- rowMeans(data_TPACK_trelis_q[, items]), # y-variabel
      y        <- 0)

   print(c(var1, "=", items))
   
   if (!length(y)==1) {                # sjekk for at vi har flere enn et item
    
      x        <- data_TPACK_trelis_q[, items]
    
      xy       <- data.frame(y,x)
      colnames(xy) <- c(var1,items)
    
      # lag tekststreng til formel y= x1 + x2 + osv.
      independent_var <- paste("'] ~ xy [,'", items[1], sep="") #TK1"
      for (item in items[2:(length(items))]) { 
        independent_var <- paste(independent_var, "'] + xy [,'", item, sep="")
      }
      independent_var <- paste(independent_var, "'] - xy  [,'", sep="")
      
      #for (item in items) {
        model <- lm( as.formula(
        paste( "xy [,'", item, independent_var, item, "']", sep="")))

        # check for outliers in residuals
        print(describe_data(model$residuals))
        
        # grense for betydningsfulle outliers / leverage; s. 78 i Christophersen, 2018
        k <- length(items) # antall uavhengige variables
        n <- nrow(xy)
        ifelse (k<10, 
                leverage <- 3*(k+1)/n,
                leverage <- 2*(k+1)/n
                )
         print(c("betydningsfull grense:", leverage))
        
        # check for normalfordeling
        # 1: normalfordeling av residuals?
        print(hist(residuals(model), col = "steelblue"))
        # 2: jevn fordeling av varians, homoskedastisitet
        print(plot(fitted(model), residuals(model)))
        print(abline(h = 0, lty = 2))
         
        teller <- teller + 1   # count number of scatterplots
      #}
    }
  }
  #datasett_index <- datasett_index + 1
#} 
print(teller) # antall plot
    
```

### Missing?

Missing data reduces the size of the dataset used for analysis, which leads to lower power (i.e., larger chances for Type II-error, that is, not detecting significant effects, obtaining false negatives). There are different advice for how to handle missing data, depending on whether they are missing at random (MAR), completely at random (MCAR) or not at random (NMAR) [@rubin_inference_1975; @schafer_missing_2002]. Data that are missing at random are correlated with the questionnaire (i.e., a question may be missing from a specific version of a questionnaire, which is in a sense random). Data that are missing completely at random, do however not correlate with anything obvious. They are in a sense, seemingly truly random, with no clear rationale for their missing other than circumstance. Data missing not at random are questions that correlate with the question itself (i.e., several respondents may avoid a specific question, i.e., about gender, there seems to be a rationale, that is not a random cause, for data missing for this specific item). 

Ways to handle missing data is predictive mean matching when less than 10 % are missing, otherwise ML-methods [@weston_brief_2006; @flora_old_2012; @watkins_exploratory_2018; @mcneish_challenging_2017; @gallagher_introduction_2013]. 

#### pilot

```{r missing_pilot}
# ref: https://gabriellajg.github.io/EPSY-579-R-Cookbook-for-SEM/lavaan-lab-12-sem-for-missing-data.html
md.pattern(data_TPACK_pilot_minuspost)
```

n = 37  
9 are missing andre_fag  
16 are missing subject and gender  

#### TRELIS_Q

```{r missing_TRELISQ}
# ref: https://gabriellajg.github.io/EPSY-579-R-Cookbook-for-SEM/lavaan-lab-12-sem-for-missing-data.html
md.pattern(data_TPACK_trelis_q)
```

At spørsmål mangler i spørreskjemaet kalles missing at random (MAR), siden missing skyldes at spørsmålene ikke var med i spørreskjemaet (jf. forklaring på MAR i Schafer og Graham, 2002, som gir nettopp dette som et eksempel på MAR). (random betyr ikke her at det er tilfeldig hvilke spørsmål som mangler, hvilket tilsvarer MCAR, men at det nettopp ikke er tilfeldig hvilke spørsmål som mangler, som kan være litt forvirrende begrepsbruk.)

På TPACK-PST mangler også 5% av spørsmålene (109 items). Ser man på mønsteret her er det 4 respondenter som stort sett ikke har gitt svar, og står for mange av disse missing (6 respondenter: Rad 34, 44-46, 95 og 102 står for 90 missing = 83%). Disse er NMAR (fordi om de svarer har med spørsmålet å gjøre) eller MAR (fordi om de svarer har med spørreskjemaet å gjøre??). Det er altså 19 MCAR (altså at 2 eller færre missing per respondent). Rad 61 er kanskje MAR, siden det er de fire siste spørsmålene som ikke har blitt besvart. Kanskje respondenten ikke gadd mer? 

```{r}
# kilde: https://www.statology.org/r-find-missing-values/
which(is.na(data_TPACK_trelis_q))
sum(is.na(data_TPACK_trelis_q))
sum(is.na(data_TPACK_trelis_q[c(34,44,45,46,95,102),]))

for (i in 1:110) {
  if (sum(is.na(data_TPACK_trelis_q[i,])) > 0) {
  print(c("rad",i))
  print(c("kolonner",which(is.na(data_TPACK_trelis_q[i,]))))
  print(c("antall", sum(is.na(data_TPACK_trelis_q[i,]))))
  print("")
}}
```

109 missing. (jf. data_screening-2023.Rmd)

På TPACK-PST mangler også 5% av spørsmålene (109 items). Ser man på mønsteret her er det 4 respondenter som stort sett ikke har gitt svar, og står for mange av disse missing (6 respondenter: Rad 34, 44-46, 95 og 102 står for 90 missing = 83%). Disse er NMAR (fordi om de svarer har med spørsmålet å gjøre) eller MAR (fordi om de svarer har med spørreskjemaet å gjøre??). Det er altså 19 MCAR (altså at 2 eller færre missing per respondent). Rad 61 er kanskje MAR, siden det er de fire siste spørsmålene som ikke har blitt besvart. Kanskje respondenten ikke gadd mer?

90 er NMAR.
19 er MCAR eller MAR.

```{r}
# clean environment to reset for next step
# ref.: https://www.statology.org/clear-environment-in-r/ [3.9.2024]
rm(list=ls())
```

## Step 6: Factor extraction (i.e., EFA)

Here I do an exploratory factor analysis (EFA) to examine how the items correlate and can be structured. The aim is to reduce the items to a limited set of factors that represent the data. 

```{r}
# load data
load(file="../data/processed_data/tpack-pilot_2022.Rdata") 
```

```{r items_EFA}
TK_pos <- c("TK1","TK2","TK3","TK4","TK5","TK6","TK7","TK8")
PK_pos <- c("PK1","PK2","PK3","PK4","PK5","PK6")
CK_pos <- c("CK1","CK2","CK3","CK4","CK5")
PCK_pos <- c("PCK1","PCK2","PCK3","PCK4","PCK5")
TCK_pos <- c("TCK1","TCK2","TCK3","TCK4")
TPK_pos <- c("TPK1","TPK2","TPK3","TPK4","TPK5")
TPACK_pos <- c("TPACK1","TPACK2","TPACK3","TPACK4","TPACK5")

#TPACK_complete <- c(TK_pos, PCK_full_pos, TPACK_full_pos) # minus XK

pilot_list <- list(TK=TK_pos,
                   CK=CK_pos,
                   PK=PK_pos,
                   PCK=PCK_pos,
                   TCK=TCK_pos,
                   TPK=TPK_pos,
                   TPACK=TPACK_pos)

```

In the following we will only consider the pre-datasett, so we remove the post-data, which is to dissimilar from the pilot data.

```{r select_data_for_screening_step6}
# emne2_anon (uten post)
data_TPACK_pilot_minuspost <- data_TPACK_pilot[
  data_TPACK_pilot$datasett != "post" & 
  data_TPACK_pilot$datasett != "pilot_trelis_q",]
```

```{r remove_collinear_items_EFA}
### pilot
data_TPACK_pilot_minuspost <- data_TPACK_pilot_minuspost %>%
    # 25.5.2023: velg bort items som ikke er i TRELIS-Q
    select(-c("TK8","PK2","PK4","PK5","PK6","PCK1","PCK3","PCK4","TCK2","TCK3",
              "TPK2","TPK3","TPK5","TPACK1","TPACK5",
              "TPACK3", # colinear in trelis-q; "TK1","TPK1",
              "TK6","TK4", "PK1", "PK3"))

```

### Describe variables

To get an overview of the quality of the data and to see what data it is appropiate to use further in the analysis.

First count distribution of answers on the likert-scale used.

```{r likert_distribution}
# kilde: https://solomonkurz.netlify.app/post/2021-05-11-yes-you-can-fit-an-exploratory-factor-analysis-with-lavaan/
data_TPACK_pilot_minuspost[,colnames(data_TPACK_pilot_minuspost) %in% unlist(pilot_list)] %>% 
  pivot_longer(everything(),values_to="value") %>% 
  count(value) %>% 
  mutate(percent = (100 * n / sum(n)) %>% round(digits = 1))
```

Plot histogram for each items, to see how they compare.

```{r histogram}
# plot multi-panel histograms with facet_wrap
# kilde: https://www.zevross.com/blog/2019/04/02/easy-multi-panel-plots-in-r-using-facet_wrap-and-facet_grid-from-ggplot2/

data_long <- pivot_longer(data_TPACK_pilot_minuspost, 
                          cols=colnames(data_TPACK_pilot_minuspost)[colnames(data_TPACK_pilot_minuspost) %in% unlist(pilot_list)],
                          names_to="item", 
                          values_to="likert_verdi",
                          values_drop_na = TRUE)

ggplot(data_long, aes(factor(likert_verdi))) +
  geom_histogram(stat = "count") + 
  facet_wrap("item")
```

<!-- Poor items: y10 (CK2), y13 (CK5) (because of range=2). -->

### Preliminary analysis

"It is recommended to carry out a preliminary analysis of the metric quality of the items to subject the most adequate items to EFA. For this purpose, it is recommended to analyze and report the mean, standard deviation, and item-test correlation of each one of the items, as well as the Cronbach’s alpha of the scales of the test. The researcher should decide whether to eliminate certain items" (Izquierdo et al, 2014, p. ??)

```{r help_function_correlation_matrix}
### HELP FUNCTION

antall_desimaler <- 2

# print effect size with p-value
print_starvalue <- function (pvalue, effectsize, bonferroni=1) {
  ifelse (pvalue < 0.001/bonferroni, star <- "***",
  ifelse (pvalue < 0.01/bonferroni, star <- "**",
  ifelse (pvalue < 0.05/bonferroni, star <- "*",
          star <- "")))
  return (paste(format(effectsize,
                       digits=antall_desimaler,
                       nsmall=antall_desimaler),
                star, sep="")) # antall desimaler=2
}


create_matrix <- function (df, method="pearson") {
  # create correlation matrix for df (and covariance matrix combined, cf. Weston, 2006, table 2, p. 736) (cov lower left, cor upper right)
  
  # since ordered data, use spearman; option in function call?
  
  # left side
  # use casewise deletion if missing
  cov_matrix     <- cov(df, use="complete.obs", method=method)
  # right side
  cor_matrix     <- data.frame(matrix(ncol = ncol(df), nrow = 0)) # correlation matrix
  
  i              <- 1          # start posisjon
  teller         <- 0          # hvor mange plot lager jeg?
  for (var1 in df[1:ncol(df)-1]) {
    i            <- i + 1
    
    # create right side: correlation 
    right_side   <- data.frame(matrix(ncol = 0, nrow = 1)) # row values
    for (var2 in df[i:ncol(df)]) {
      teller     <- teller + 1   
      cor_var1_var2 <- cor.test(var1,var2, method=method)
      est_p      <- print_starvalue(cor_var1_var2$p.value,
                                    cor_var1_var2$estimate)
      right_side <- cbind(right_side,est_p) # add column
    }
    
    # add left-side of matrix: covariance
    left_side    <- format(cov_matrix[i-1,1:i-1],
                           digits=antall_desimaler)
    
    temp_row     <- c(left_side, right_side)  # merge left and right
    names(temp_row) <- colnames(df)
    cor_matrix   <- rbind(cor_matrix, temp_row)   # add row
  }
  
  # add final row
  final_row      <- format(cov_matrix[i,1:i],digits=antall_desimaler)
  names(final_row) <- colnames(df)
  cor_matrix     <- rbind(cor_matrix, final_row)
  
  # add row with mean and sd based on rows/persons
  row_mean       <- round(colMeans(df, na.rm=TRUE),
                          digits=antall_desimaler)
  row_sd         <- format(psych::SD(df),
                          digits=antall_desimaler)
  cor_matrix     <- rbind(cor_matrix,"mean"=row_mean)
  cor_matrix     <- rbind(cor_matrix,"sd"=row_sd)
  
  # set names of columns and rows
  row.names(cor_matrix) <- c(colnames(df),"mean","sd")
  return(cor_matrix)
}

lag_eigenvalue_tabell <- function(ev_list) {
  no_ev <- length(ev_list) # antall eigenvalues
  kum_var <- 0
  
  # gjør om til tabell
  ev_tabell <- data.frame(matrix(ncol=4,nrow=0))
  for (i in 1:no_ev) {
    ev <- ev_list[i]
    kum_var <- kum_var + ev/no_ev
    faktor <- cbind(as.integer(i), ev, ev/no_ev, kum_var)
    #print(faktor)
    ev_tabell <- rbind(ev_tabell, faktor)
  }
  
  colnames(ev_tabell) <- c("Factor","Eigenvalue","Variance","Cummulativ variance")
  
  pander(format(ev_tabell,digits=2))
}

```

```{r calculate_mean_sd}
# select Likert columns
data_pilot <- data_TPACK_pilot_minuspost[,colnames(data_TPACK_pilot_minuspost) %in% unlist(pilot_list)]

# remove items with over 50 % NA
df_noNA <- data_pilot[ , colSums(is.na(data_pilot)) < nrow(data_pilot)/2] 
```

#### eigenvalues

```{r}
# check for collinearity; values near or below 0
ev <- eigen(cor(df_noNA[,colnames(df_noNA) %in%
                                unlist(pilot_list)],
                use="pairwise.complete.obs"), only.values=TRUE)

lag_eigenvalue_tabell (ev$values)
```

#### item-test correlation

Check to see if an item corresponds to the average test-result for the intended factor (e.g., if the TK-items correspond to the other TK-items). 

Ref.: https://www.questionmark.com/resources/blog/psychometrics-101-item-total-correlation/ https://en.wikipedia.org/wiki/Item-total_correlation 

```{r item_test}
# get data for each test / construct
TK    <- df_noNA[,colnames(df_noNA) %in% TK_pos]
PK    <- df_noNA[,colnames(df_noNA) %in%PK_pos]
CK    <- df_noNA[,colnames(df_noNA) %in%CK_pos] # NA
TCK   <- df_noNA[,colnames(df_noNA) %in%TCK_pos]
TPK   <- df_noNA[,colnames(df_noNA) %in%TPK_pos]
PCK   <- df_noNA[,colnames(df_noNA) %in%PCK_pos]
TPACK <- df_noNA[,colnames(df_noNA) %in% TPACK_pos]
 
# check correlation of items to construct
pander(create_matrix(data.frame(TK,    "TK"=rowMeans(TK, na.rm=TRUE))))
pander(create_matrix(data.frame(PK,    "PK"=rowMeans(PK, na.rm=TRUE))))
pander(create_matrix(data.frame(CK,    "CK"=rowMeans(CK, na.rm=TRUE))))
pander(create_matrix(data.frame(TCK,   "TCK"=rowMeans(TCK, na.rm=TRUE))))
pander(create_matrix(data.frame(TPK,   "TPK"=rowMeans(TPK, na.rm=TRUE))))
pander(create_matrix(data.frame(PCK,   "PCK"=rowMeans(PCK, na.rm=TRUE))))
pander(create_matrix(data.frame(TPACK, "TPACK"=rowMeans(TPACK, na.rm=TRUE))))

```

TCK2 has low item-test correlation. 

#### Reliability: Cronbach's alpha

Assumptions for calculating Cronbach's alpha: uni-dimensionality (one factor = factor analysis) and tau-equality (equal covariance) [@revelle_reliability_2019].

##### TK

```{r uni-dimensionality}
parallel <- fa.parallel(TK,fm='uls',fa='fa',SMC=TRUE) #, cor="poly", correct=0 ,use="complete.obs") #)
parallel$fa.values
```

```{r tau-equality, warning=T}
pander(create_matrix(TK))
```

Forutsetninger ikke oppfylt: ikke uni-dimensional (2 faktorer fra parallell analysis, og 1 fra eigenvalue), og ikke tau-equality (TK8 passer ikke på tau-equality).

```{r alpha}
psych::alpha(TK)
```

##### CK

```{r warning=FALSE}
parallel <- fa.parallel(CK,fm='uls',fa='fa',SMC=TRUE,  cor="poly", correct=0,use="complete.obs") #,cor="poly")
parallel$fa.values
pander(create_matrix(CK))
psych::alpha(CK) 
```

Uni-dimensional, og nesten tau-equality. CK4 "ødelegger". CK4 er allerede tagget for fjerning.

##### PK

```{r warning=FALSE,eval=F}
parallel <- fa.parallel(PK,fm='uls',fa='fa',SMC=TRUE,  cor="poly", correct=0,use="complete.obs") #,cor="poly")
parallel$fa.values
pander(create_matrix(PK))
psych::alpha(PK)

```

Uni-dimensional. Bortimo tau-equality. PK4 og PK5 "ødelegger" litt. Begge disse er allerede tagget for fjerning. PK6 senker også reliabiliteten. (Denne er dog tatt med videre til TRELIS-q)

##### TCK

```{r warning=FALSE}
parallel <- fa.parallel(TCK,fm='uls',fa='fa',SMC=TRUE,  cor="poly", correct=0,use="complete.obs") #,cor="poly")
parallel$fa.values
pander(create_matrix(TCK))
psych::alpha(TCK)

```

Uni-dimensional. Ikke tau-equality. TCK1 ødelegger. 

##### TPK

```{r warning=FALSE,eval=T}
parallel <- fa.parallel(TPK,fm='uls',fa='fa',SMC=TRUE) #,  cor="poly", correct=0,use="complete.obs") #,cor="poly")
parallel$fa.values
pander(create_matrix(TPK))
psych::alpha(TPK)

```

Uni-dimensional og omtrent tau-equality.

##### PCK

```{r warning=FALSE}
parallel <- fa.parallel(PCK,fm='uls',fa='fa',SMC=TRUE,  cor="poly", correct=0,use="complete.obs") #,cor="poly")
parallel$fa.values
pander(create_matrix(PCK))
psych::alpha(PCK)

```

Uni-dimensional og omtrent tau-equality. PCK1 og PCK3 kan fjernes. (men PCK1 er tatt med videre)

##### TPACK

```{r warning=FALSE, eval=T}
  parallel <- fa.parallel(TPACK,fm='uls',fa='fa',SMC=TRUE,
                          cor="poly", correct=0,use="complete.obs") #,cor="poly")
  parallel$fa.values
  pander(create_matrix(TPACK))
  psych::alpha(TPACK)

```

Uni-dimensional og halvveis tau-equality. TPACK1 "ødelegger" litt.

#### Results of preliminary analysis

TK7 and CK4 stand apart as poor items.

Reset data for EFA

```{r}
# remove columns
data_modified <- within(data_TPACK_pilot_minuspost, rm("TK7","CK4")) 

# pick Likert columns
df <- data_modified[,colnames(data_modified) %in% unlist(pilot_list)]

# remove respondents with more than 50% NA
df_run1 <- df[ , colSums(is.na(df)) < nrow(df)/2] 

```

### EFA 

The aim of the exploratory factor analysis is to find the lowest number of items that is needed to represent the dataset.

Estimation is done using maximum-likelihood estimation. Rotation is oblique (oblimin). tenBerge is used to estimate scores (ref. Psych-package help-file).

#### run 1

The suitability of the data for doing an EFA is tested by calculating KMO og Bartlett.

```{r}
# KMO og Bartlett
KMO_base      <- KMO(df_run1) # addition of PCK1 changed KMO from 0.5 to 0.36
bartlett_base <- cortest.bartlett(df_run1, n=5,diag=TRUE)
```

The number of factors is estimated using parallel analysis [@worthington_scale_2006].

```{r parallel_analysis_run1}
# number of factors: eigenvalue, scree or parallel (or theoretical)
# polychoric is used for ordered data with skewness, Watkins (2018)

parallel_base <- fa.parallel(df_run1,fm='mle',fa='fa',SMC=TRUE,  use="complete.obs") #, )correct=0,cor="poly",

#parallel <- fa.parallel(df,fm='alpha',fa='fa',nfactors=7,use="complete.obs",cor='poly', error.bars=TRUE, SMC=TRUE) # complete.obs = delete NA; poly for fa recommended av Revelle & Condon, 2019
```

```{r}
# eigenvalues
lag_eigenvalue_tabell (parallel_base$fa.values)

print(format(list("KMO" = KMO_base$MSA, "Bartlett p =" = bartlett_base$p.value),digits=2))
```

Due to the ambiguity of the parallel analysis, several factor solutions are tested (i.e., 2-factor, 3-factor and 4-factor).

##### 2-factor

```{r efa_ordinal2}
# FA
# ref.: https://www.rdocumentation.org/packages/psych/versions/2.2.5/topics/fa

# print covariation matrix all items
# bruk fm=uls, ikke pa (Flora et al, 2012)
# bruk polykorisk korrelasjon (Flora et al, 2012)
# use covariance matrix (correlation matrix fails because of missing)

model_2factor_run1 <- fa(df_run1, nfactors = 2,rotate = "oblimin", fm="mle", scores="tenBerge") # missing=TRUE,impute="mean", ,oblique.scores=TRUE, cor="poly", correct=0 covar=TRUE,

print.psych(fa.sort(model_2factor_run1))
```

##### 3-factor

```{r efa_ordinal3}
# FA
# ref.: https://www.rdocumentation.org/packages/psych/versions/2.2.5/topics/fa

# print covariation matrix all items
# bruk fm=uls, ikke pa (Flora et al, 2012)
# bruk polykorisk korrelasjon (Flora et al, 2012)
# use covariance matrix (correlation matrix fails because of missing)

model_3factor_run1 <- fa(df_run1, nfactors = 3,rotate = "oblimin", fm="mle", scores="tenBerge") # missing=TRUE,impute="mean", covar=TRUE,,oblique.scores=TRUE, cor="poly", correct=0

print.psych(fa.sort(model_3factor_run1))
```

##### 4-factor

```{r efa_ordinal4}
# FA
# ref.: https://www.rdocumentation.org/packages/psych/versions/2.2.5/topics/fa

# print covariation matrix all items
# bruk fm=uls, ikke pa (Flora et al, 2012)
# bruk polykorisk korrelasjon (Flora et al, 2012)
# use covariance matrix (correlation matrix fails because of missing)

model_4factor_run1 <- fa(df_run1, nfactors = 4,rotate = "oblimin", fm="mle", scores="tenBerge") # missing=TRUE,impute="mean", covar=TRUE,,oblique.scores=TRUE, cor="poly", correct=0

print.psych(fa.sort(model_4factor_run1))
```

```{r}
# eigenvalues
print(model_3factor_run1$values)
```

Eigenvalues suggests 3 factors

The cutoff for significant loadings ...

```{r}
# cutoff verdi gitt av formel i Watkins, 2018 (avsnitt: Interpretation of results)
n <- model_3factor_run1$n.obs
c <- 4.084/sqrt(n-2)   # n=100 gir 3.92 p<5% (z-dist); n=30 gir 4.084 for p<5% (t-dist)
print(c)
# bare print loadings med verdier over 0.32
print(model_3factor_run1$loadings,cutoff = c)

print.psych(model_3factor_run1)
```

Weak communality ...

```{r communality}
# h2 = communality; bare print items med communality under 0.4 siden disse kan fjernes (Worthington, 2006); eller 0.6 for små utvalg? (ref)
# kilde: https://stackoverflow.com/questions/30467834/r-print-cutoff-values-under-a-certain-value
print(model_3factor_run1$communality[model_3factor_run1$communality<0.6])
```

Remove poor items from run 1 due to cross-loadings ...

```{r}
# 26.5.2023, remove cross-loadings
df_run2 <- within(df_run1, rm("TK5"))
```

Kan vurdere å beholde CK1 med h2 = 0.37.

#### run 2

The EFA is repeated, since an item was removed. 

Suitability of the data for doing an EFA.

```{r}
# KMO og Bartlett
KMO_base      <- KMO(df_run2) # addition of PCK1 changed KMO from 0.5 to 0.36
bartlett_base <- cortest.bartlett(df_run2, n=4,diag=TRUE)
```

```{r parallel_analysis_run2}
# fm = mle, alpha, pa

#if (do_parallel == 1) {

# number of factors: eigenvalue, scree or parallel (or theoretical)
# parallel analysis (Worthington, 2006)
# polychoric is used for ordered data with skewness, Watkins (2018)
  # "pa (parallel analysis) most accurately estimated the number of factors" (Li et al., 2020)

parallel_base <- fa.parallel(df_run2,fm='mle',fa='fa',SMC=TRUE) #, ),  correct=0,cor="poly",use="complete.obs"
#parallel <- fa.parallel(df,fm='alpha',fa='fa',nfactors=7,use="complete.obs",cor='poly', error.bars=TRUE, SMC=TRUE) # complete.obs = delete NA; poly for fa recommended av Revelle & Condon, 2019
```

```{r}
# eigenvalues
#parallel$fa.values
lag_eigenvalue_tabell (parallel_base$fa.values)

print(format(list("KMO" = KMO_base$MSA, "Bartlett p =" = bartlett_base$p.value),digits=2))
```

##### 2 factor

```{r}
model_2factor_run2 <- fa(df_run2, nfactors = 2, rotate = "oblimin", fm="mle", scores="tenBerge") # ,oblique.scores=TRUE, cor="poly", correct=0

print.psych(fa.sort(model_2factor_run2))
```

##### 3 factor

```{r}
# equamax for complex structure, ref. Schmitt & Sass, 2011
# CI with bootstrapping, n.iter=1000
# p = .8 (Cohen, 1990)
model_3factor_run2 <- fa(df_run2, nfactors = 3, rotate = "oblimin", fm="mle", scores="tenBerge") #, missing=TRUE, impute="mean") # n.iter=1000, p=.8,  #,oblique.scores=TRUE, cor="poly", correct=0)

print.psych(fa.sort(model_3factor_run2))

#factor_model2
```

##### 4 factor

```{r}
model_4factor_run2 <- fa(df_run2, nfactors = 4, rotate = "oblimin", fm="mle", scores="tenBerge") #,oblique.scores=TRUE, cor="poly", correct=0)

print.psych(fa.sort(model_4factor_run2))
```

Check 2nd order solution.

```{r}
# korrelation matrix factors
#format(round(factor_model$r.scores,1), nsmall=1) 
order2 <- model_3factor_run2$Phi # 2nd order factors
print(order2)
```

```{r}
# eigenvalues
print(model_3factor_run2$values)              

# loadings
print.psych(model_3factor_run2) #, cut=c)
```

Remove items "with factor loadings less than .32 or cross-loadings less than .15 difference from an item’s highest factor loading. In addition, they should also delete items that contain absolute loadings higher than a certain value (e.g., .32) on two or more factors." (Worthington and Whittaker, 2006).

#### Assessment

Felles varians: hvor mye felles varians forklarer faktorene? Faktorene bør tilsammen forklare minst 50% av variansen (Christophersen, 2018).

```{r}
p <- model_3factor_run2$Vaccounted
print(p)                           # se på cumulative var
```

Kommunaliteten (h2) beskriver hvor stor del av variansen til indikatoren som faktorene "forklarer". Residualvariansen (u2) er den uforklarte variansen til indikatoren (item/spørsmålet). Det er i praksis vanskelig å få særlig høyere kommunalitet enn 65% i følge Christophersen (2018).

```{r communality7}
# Mundfrom et al., 2005: Råd: høy kommunalitet (>0.6) 
# dette er unstandardized loadings / covariance matrix ??
print(model_3factor_run2$communality[model_3factor_run2$communality>0.6])
# dette er standardized loadings ??
#print(efa7_ml_pro$communalities[efa7_ml_pro$communalities>0.6])

# avoid below 0.4
print(model_3factor_run2$communality[model_3factor_run2$communality<0.4])
```

Vurder modellen: Hvor sterkt korrelerer indikatorene i modellen? Indikatorer med r>0.80 er for sterkt korrelert (Christophersen, 2018). Indikatorer som korrelerer svært svakt kan tyde på at et vesentlig begrepsaspekt er utelatt av modellen.

Når det er korrelerte faktorer vi bruker, så vil strukturmatrisen gi korrelasjonene, mens pattern matrisen gir loadings.

```{r}
# for sterke korrelasjoner
print(model_3factor_run2$Structure, cutoff=0.8)

model_3factor_run2$Structure

# meget svake korrelasjoner
#efa7_ml_pro$Structure[efa7_ml_pro$Structure<0.1]
```

restledd skal helst ikke være større enn +- 0.05

```{r}
round(model_3factor_run2$residual, digits=2)
```

```{r}
# check reliability
#cronbach.alpha(df_run2, na.rm=TRUE)

# tegn factor_model
stimodel <- fa.diagram(model_3factor_run2)
#print(stimodel)
```

```{r}
# compare with intervall pearson (appx normal distributed)
pander(create_matrix(df_run2))
```

### McDonald's omega

Fra help-file til omega: "A recommendation that should be heeded, regardless of the method chosen to estimate \omega_hω, is to always examine the pattern of the estimated general factor loadings prior to estimating \omega_hω. Such an examination constitutes an informal test of the assumption that there is a latent variable common to all of the scale's indicators that can be conducted even in the context of EFA. If the loadings were salient for only a relatively small subset of the indicators, this would suggest that there is no true general factor underlying the covariance matrix. Just such an informal assumption test would have afforded a great deal of protection against the possibility of misinterpreting the misleading \omega_hω estimates occasionally produced in the simulations reported here." (Zinbarg et al., 2006, p 137)"

```{r eval=T}
alpha_TK <- alpha(df_run2[,c("TK1", "TK2", "TK3")])
alpha_PCK <- alpha(df_run2[,c("CK1", "PCK2", "PCK5", "CK5", "CK3", "CK2")])
alpha_TPACK <- alpha(df_run2[,c("TPK1","TPK4","TCK1","TCK4", "TPACK2","TPACK4")]) # , "TPK4"

alpha_TK
alpha_PCK
alpha_TPACK
```

```{r}
omega(df_run2, fm="mle") #, rotate="Promax")
```

# Results of EFA

Here I reproduce the results presented in paper 1.

table 1

```{r}
print.psych(fa.sort(model_3factor_run2))
```

```{r}
alpha_TK$total$std.alpha
alpha_PCK$total$std.alpha
alpha_TPACK$total$std.alpha
```



```{r}
# clean environment to reset for next step
# ref.: https://www.statology.org/clear-environment-in-r/ [3.9.2024]
rm(list=ls())
```

# Stage 3 

From stage 3 there are three steps which will be discussed: step 7 (tests of dimensionality, i.e., CFA), step 8 (tests of reliability) and step 9 (tests of validity). 

## Step 7: Tests of dimensionality (i.e., CFA)

The following choices were made when determining how to test the data:

* treat the data as interval data.
* missing data: use maximum-likelihood [@mcneish_challenging_2017; @schafer_missing_2002].
* Normal or nonnormal distribution of residuals? Normality is an important assumption for doing test statistics. Nonnormal means skew>2 and kurtosis>9 (ref?). Some of the residuals showed high skewness (i.e., non-normality) (e.g., TPACK2). In such cases, WLS can be an option to OLS [@christophersen_introduksjon_2018]. Some items also showed evidence of being homoscedastic (i.e., non-equal variance in the residuals): CK123, PCK5, TCK4. In case of nonnormal data the Satorra-Bentler scaled chi-square test statistic and robust SE can be used [@flora_old_2012; @mcneish_challenging_2017].
* I aim for approximate fit, not exact fit [@flora_old_2012; weston_brief_2006].

yuan.bentler forutsetter adf-estimering. (McNeish, 2017) som krever at utvalgsstørrelser er minst p(p+1)/2, der p er antall items i modellen. Med p=13 blir det 91 respondenter. Mitt datautvalg er større enn dette (n=103).

test="yuan.bentler(.mplus)" (McNiesh, 2017: small samples? NB: adf)  
se="robust.sem" # standard error  
information="observed" # ref. Schafer & Graham, 2002, p. 162  

### Specification of models to be tested

The purpose is to confirm the measurement model, that is, the indicators loading to the latent factors. 

The two models are based on the EFA and are nested (e.g., the two_factor model is a constrained version of the three_factor model, i.e., TK and TPACK are merged). 

```{r specify_models}
three_factor <- '
PCK   =~ CK123 + PCK2 + PCK5 + CK5 
TK    =~ TK1b + TK2 + TK34 
TPACK =~ TPACK2 + TPACK4 + TCK4 + TPK4 + TCK1 + TPK1
'

two_factor   <- '
PCK   =~ CK123 + PCK2 + PCK5 + CK5
TPACK =~ TK1b + TK2 + TK34 + TPACK2 + TPACK4 + TCK4 + TPK4 + TCK1 + TPK1
'
```

### Test of models

Two models are compared. A two factor and a three factor solution derived from the EFA.

```{r}
# load data
load(file="../data/processed_data/tpack-trelis_q_partial_2023-paper1.Rdata")
```

NMAR are removed from the dataset.

```{r remove_NMAR_TRELISQ}
# remove NMAR (19)
CFA_TPACK_PST <- data_TRELIS_Q[-c(19,34,44,45,46,95,102),] 
```

Some help functions are defined. 

```{r help_functions_cfa}

### HELP FUNCTIONS
do_cfa        <- function (cfaspec, cfadata) {
  # estimator og missing: Rhemtulla et al. (2012)
  # ref.: https://statistics.ohlsen-web.de/multiple-imputation-with-mice/
  TPACK_PST_0 <- cfa(cfaspec, data= cfadata, std.lv=TRUE,
                     missing="fiml", #robust.two.stage",
                     estimator="ML", #missing="pairwise", 
                     #test="satorra.bentler", 
                     test="yuan.bentler",
                     information="observed",
                     se="robust.huber.white") # robust.sem

  return (TPACK_PST_0)
}


print_model <- function (cfamodel) {
  # print results of CFA
  
  print("summary")
  print(summary(cfamodel, standardized=TRUE, fit.measures=TRUE)) 
  
  print("estimates")
  # CI suggested by Cheung et al. (2023) for discriminant validity < 1 (one-tailed)
  print(parameterEstimates(cfamodel, rsquare = TRUE, level=0.90))
  
  print("modindices")
  print(modindices(cfamodel, sort=TRUE, maximum.number = 5))            # mi should be below 3.84
  
  print("reliability")
  rel <- compRelSEM(cfamodel, dropSingle = FALSE)
  print(rel)
  return (rel)   # for later comparison of reliability between models
}

```

#### two_factor

```{r estimate_2factor}
# egenfaktor
model_two_factor <- do_cfa(two_factor, CFA_TPACK_PST)
rel_two_factor   <- print_model(model_two_factor)
```

#### three_factor

```{r estimate_3factor}
# egenfaktor
model_three_factor <- do_cfa(three_factor, CFA_TPACK_PST)
rel_three_factor   <- print_model(model_three_factor)
```

### Summary of fit

```{r CFA_results_comparison}
# fit indices
columnnames  <- c("AIC", "chi-square", "df", "p-value",
                  "RMSEA", "RMSEA (90%) CI lower","RMSEA (90%) CI upper",
                  "CFI","TLI", "SRMR") #, "chi2/df", 
                 #"chi-square scaled Swain", "p-value Swain", 
                 #"swain chi2/df")

# fit criteria
fit_criteria <- c("x", "x", "x", ">0.05", 
                  "<0.06(0.05-0.10)", "x", "x",
                  ">0.95(0.9-0.95)", ">0.95", "<0.08(0.08-0.15)") #, "<2", "x", ">0.05", "<2") # Weston and Gore (2006); Gallagher and Brown (2013)

fit_2factor  <- round(fitMeasures(model_two_factor,
                  c("aic","chisq.scaled", "df.scaled",
                    "pvalue.scaled",
                    "rmsea.robust", "rmsea.ci.lower.robust", "rmsea.ci.upper.robust",
                    "cfi.robust","tli.robust","srmr")),3)

fit_3factor  <- round(fitMeasures(model_three_factor,
                  c("aic","chisq.scaled", "df.scaled",
                    "pvalue.scaled",
                    "rmsea.robust", "rmsea.ci.lower.robust", "rmsea.ci.upper.robust",
                    "cfi.robust","tli.robust","srmr")),3)

rownames     <- c("lower bounds", "two_factor", "three_factor")

fit_table    <- rbind(fit_criteria, fit_2factor)
fit_table    <- rbind(fit_table, fit_3factor)

colnames(fit_table)  <- columnnames
row.names(fit_table) <- rownames

pander(fit_table)
```

### Results

The correlation matrix helps assess the data.

```{r covariance_matrix}
# covariance matrix / "TK1a","TK567"
pander(create_matrix(CFA_TPACK_PST[,c("CK123", "CK5", "PCK2" , "PCK5" , "TK1b", "TK2", "TK34", "TCK1","TCK4", "TPK1","TPK4","TPACK2","TPACK4")])) # ,"TPK4"
```

To calculate the students TPACK-score we would only calculate the average of all the items, i.e. an integrative TPACK-model, where all the elements contributes to the students TPACK. 

```{r scores}
TK    <- c("TK1b", "TK2", "TK34") # "TK1a","TK567"
PCK   <- c("CK123", "PCK2" , "PCK5" , "CK5")
TPACK <- c("TPK1","TPK4","TPACK2","TCK1","TPACK4", "TCK4") # , "TPK4"

TPACK_PST_items <- list("TK" = TK,
                        "PCK" = PCK,
                        "TPACK" = TPACK) # everyone contributes til TPACK; c(TK,"TCK4",TPK, "CK123","PCK2","PCK5"))

df_CFA    <- CFA_TPACK_PST[,unlist(TPACK_PST_items[1:3])]
describe(df_CFA)      # with missing (see n)? (na.rm=TRUE)

score_TPACK <- scoreItems(TPACK_PST_items,df_CFA)
describe(score_TPACK$scores)
```

## Step 8: Tests of reliability

Three models are tested for reliability. The CFA-model (the estimated correlation matrix), the correlation matrix from the observed variables, and the observed variables.

```{r omega_mle4}
omega0  <- omegaFromSem(model_three_factor)
omega0

# get correlation matrix of observed variables
cor_mat <- lavInspect(model_three_factor, what="cor.ov")
observed_omega <- omega(cor_mat, nfactors=3)
observed_omega

omega(CFA_TPACK_PST[,c("CK123", "CK5", "PCK2" , "PCK5" , "TCK1","TCK4", "TPK1","TPK4","TPACK2","TPACK4", "TK1b", "TK2", "TK34")]) # ,"TPK4"
```

## Step 9: Tests of validity

I test for discriminatory and convergent validity following @ronkko_updated_2022

```{r}
# ref.: https://link.springer.com/article/10.1007/s10490-023-09871-y
source("../../scripts/measureQ.R")
```

```{r measureQ, eval=T}
table.2 <- measureQ(model_three_factor, CFA_TPACK_PST)
```

# Results of CFA

Table 2

```{r}
pander(fit_table)
```

Figure 3

std.all (column to the far right) shows the standardized results used in figure 3.

```{r}
parameterEstimates(model_three_factor, standardized=TRUE)
```

Table 3

```{r}
describe(df_CFA)
```

F1 equals TPACK, F2 equals PCK and F3 equals TK. Omega_t is the first column, while omega_h is the second column.

```{r}
observed_omega$omega.group
#observed_omega$omega.group$general
#observed_omega$omega.group$total
```

Table 4

```{r}

so.lv <- NULL # temp variable second order level

cat("\n", "  Table 2. Descriptive Statistics (Observed Mean, Latent s.d., AVE, Construct Reliability, Latent Correlation)", rep("\n", 2))
rownames(table.2) <- rep("    ", nrow(table.2))
print(table.2, quote=FALSE, right=TRUE)
cat("\n", "  Note: AVE = Average Variance Extracted; * = AVE significantly lower than 0.5 (p < .05)","\n")
if (length(so.lv) == 0) {
  cat("         diagonal elements in brackets = Construct Reliability","\n")
} else {
  cat(paste0("         diagonal elements in brackets = Construct Reliability for first-order factor and ", omega, " for second-order factor"), "\n")
}
cat("         A = Construct Reliability significantly lower than 0.7; B = Construct Reliability significantly lower than 0.8 (p < .05)", "\n")
cat("         Correlation coefficient: a = significantly larger than 0.85; b = significantly larger than 0.8; c = significantly larger than 0.7 (p < .05)", "\n")
if (length(so.lv) == 0) {
  cat("         # = AVE is significantly less than squared-correlation (p < .05)", rep("\n", 3))
} else {
  cat("         # = AVE is significantly less than squared-correlation (p < .05)", rep("\n"))
  cat("         Observed mean of second-order factor is based on all items ignoring the second-order structure", rep("\n", 3))
}
```

# Discussion and conclusion

The results are discussed in @karlsen_assessing_2024.

# References

<!-- citation(package) -->

<div id="refs"></div>

<!-- source: https://bookdown.org/yihui/rmarkdown-cookbook/bibliography.html 

# (APPENDIX) Appendix {-} -->

# Session info
<!-- # ref.: https://intro2r.com/proj_doc.html -->
<!-- #sessionInfo() -->

```{r}
xfun::session_info()
```
